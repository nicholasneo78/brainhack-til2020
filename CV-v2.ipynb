{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CV-v2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"19G7Cy8hnmTg1vUqFt-pKc5Ypce8hhKzS","authorship_tag":"ABX9TyOd8uiuR+xOZBZGCT9EGTGk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"PFppAGwlE9QO","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hj_lRwSVfll2","colab_type":"code","outputId":"6c2a9f92-411d-4a06-9b57-3dd442cdbee3","executionInfo":{"status":"ok","timestamp":1592322294385,"user_tz":-480,"elapsed":10751,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"17512883430045220190"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["!pip install numpy==1.17.4\n","np.__version__"],"execution_count":46,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: numpy==1.17.4 in /usr/local/lib/python3.6/dist-packages (1.17.4)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'1.17.4'"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"UT3GnHJaEziz","colab_type":"code","outputId":"6bbc0ac1-342b-4c65-c508-a04ff4742b31","executionInfo":{"status":"ok","timestamp":1592322298574,"user_tz":-480,"elapsed":1927,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"17512883430045220190"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":47,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5o0x2LsYHnVr","colab_type":"text"},"source":["## Test the file directory of google drive with dummy csv"]},{"cell_type":"code","metadata":{"id":"EvopVDVaGga3","colab_type":"code","outputId":"a50492e1-16db-4397-b44e-87d3cf621a77","executionInfo":{"status":"ok","timestamp":1592322303268,"user_tz":-480,"elapsed":1579,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"17512883430045220190"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["os.getcwd()"],"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"rfMFntMpGafu","colab_type":"code","colab":{}},"source":["a = pd.read_csv(\"./gdrive/My Drive/datasets/dummy.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BM2aUBUxGpfL","colab_type":"code","outputId":"dcfb2170-0348-41a9-c1b0-410c67933bad","executionInfo":{"status":"ok","timestamp":1592322305187,"user_tz":-480,"elapsed":1010,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"17512883430045220190"}},"colab":{"base_uri":"https://localhost:8080/","height":206}},"source":["a.head(5)"],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>shopid</th>\n","      <th>userid</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6042309</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>104804492</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8715449</td>\n","      <td>9753706</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>190969466</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2859407</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      shopid   userid\n","0    6042309        0\n","1  104804492        0\n","2    8715449  9753706\n","3  190969466        0\n","4    2859407        0"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"k9y4D5IfHuVI","colab_type":"text"},"source":["## Actual CV Task"]},{"cell_type":"code","metadata":{"id":"RouIPsTCHwma","colab_type":"code","colab":{}},"source":["import os\n","import PIL\n","import json\n","import pickle\n","import numpy as np\n","from math import log, exp\n","from tqdm import tqdm\n","from random import shuffle\n","from PIL import ImageEnhance, ImageFont, ImageDraw\n","from IPython.display import Image, display\n","from multiprocessing import Pool\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.python.keras.utils.data_utils import Sequence"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iHSU5DBZaEg0","colab_type":"code","colab":{}},"source":["from tensorflow.keras.applications.inception_v3 import InceptionV3\n","from tensorflow.keras.applications import InceptionResNetV2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-rrTqzDITkg","colab_type":"code","colab":{}},"source":["cat_list = ['tops', 'trousers', 'outerwear', 'dresses', 'skirts']\n","\n","input_shape = (224,224,3)\n","wt_decay = 5e-4\n","dims_list = [(7,7),(14,14)]\n","aspect_ratios = [(1,1), (1,2), (2,1)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q01wn6aNIV5b","colab_type":"code","colab":{}},"source":["# Set up directory\n","data_folder = './gdrive/My Drive/datasets/'\n","submission_folder = os.path.join(data_folder, 'submissions/')\n","train_imgs_folder = os.path.join(data_folder,'train/', 'train/')\n","train_annotations = os.path.join(data_folder,'train.json')\n","val_imgs_folder = os.path.join(data_folder,'val/','val/')\n","val_annotations = os.path.join(data_folder,'val.json')\n","train_pickle = os.path.join( data_folder, 'train.p/','train.p')\n","val_pickle = os.path.join( data_folder, 'val.p/','val.p')\n","\n","save_model_folder = submission_folder\n","load_model_folder = submission_folder"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nBuDUwC5IdYn","colab_type":"code","outputId":"132d4b68-49db-4c71-8115-437ca7afead7","executionInfo":{"status":"ok","timestamp":1592322321056,"user_tz":-480,"elapsed":1234,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"17512883430045220190"}},"colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["# Check if the directories are correct\n","print(train_imgs_folder)\n","print(train_annotations)\n","print(val_imgs_folder)\n","print(val_annotations)\n","print(train_pickle)\n","print(val_pickle)\n","print(submission_folder)\n","print(save_model_folder)\n","print(load_model_folder)"],"execution_count":55,"outputs":[{"output_type":"stream","text":["./gdrive/My Drive/datasets/train/train/\n","./gdrive/My Drive/datasets/train.json\n","./gdrive/My Drive/datasets/val/val/\n","./gdrive/My Drive/datasets/val.json\n","./gdrive/My Drive/datasets/train.p/train.p\n","./gdrive/My Drive/datasets/val.p/val.p\n","./gdrive/My Drive/datasets/submissions/\n","./gdrive/My Drive/datasets/submissions/\n","./gdrive/My Drive/datasets/submissions/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U87MX6fsIgC-","colab_type":"code","outputId":"414a2118-0218-4cec-c34a-fd7057dc326f","executionInfo":{"status":"ok","timestamp":1592322321059,"user_tz":-480,"elapsed":957,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"17512883430045220190"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["tf.__version__"],"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.2.0'"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"markdown","metadata":{"id":"FwhKCBvlIyu_","colab_type":"text"},"source":["## Data Augmentation Methods"]},{"cell_type":"code","metadata":{"id":"gEB7brBFIjBK","colab_type":"code","colab":{}},"source":["# Helper methods: Computes the boundary of the image that includes all bboxes\n","\n","def compute_reasonable_boundary(labels):\n","    bounds = [ (x-w/2, x+w/2, y-h/2, y+h/2) for _,x,y,w,h in labels]\n","    xmin = min([bb[0] for bb in bounds])\n","    xmax = max([bb[1] for bb in bounds])\n","    ymin = min([bb[2] for bb in bounds])\n","    ymax = max([bb[3] for bb in bounds])\n","    return xmin, xmax, ymin, ymax\n","\n","def aug_horizontal_flip(img, labels):\n","    flipped_labels = []\n","    for c,x,y,w,h in labels:\n","        flipped_labels.append( (c,1-x,y,w,h) )\n","    return img.transpose(PIL.Image.FLIP_LEFT_RIGHT), np.array(flipped_labels)\n","\n","def aug_crop(img, labels):\n","    # Compute bounds such that no boxes are cut out\n","    xmin, xmax, ymin, ymax = compute_reasonable_boundary(labels)\n","    # Choose crop_xmin from [0, xmin]\n","    crop_xmin = max( np.random.uniform() * xmin, 0 )\n","    # Choose crop_xmax from [xmax, 1]\n","    crop_xmax = min( xmax + (np.random.uniform() * (1-xmax)), 1 )\n","    # Choose crop_ymin from [0, ymin]\n","    crop_ymin = max( np.random.uniform() * ymin, 0 )\n","    # Choose crop_ymax from [ymax, 1]\n","    crop_ymax = min( ymax + (np.random.uniform() * (1-ymax)), 1 )\n","    # Compute the \"new\" width and height of the cropped image\n","    crop_w = crop_xmax - crop_xmin\n","    crop_h = crop_ymax - crop_ymin\n","    cropped_labels = []\n","    for c,x,y,w,h in labels:\n","        c_x = (x - crop_xmin) / crop_w\n","        c_y = (y - crop_ymin) / crop_h\n","        c_w = w / crop_w\n","        c_h = h / crop_h\n","        cropped_labels.append( (c,c_x,c_y,c_w,c_h) )\n","\n","    W,H = img.size\n","    # Compute the pixel coordinates and perform the crop\n","    impix_xmin = int(W * crop_xmin)\n","    impix_xmax = int(W * crop_xmax)\n","    impix_ymin = int(H * crop_ymin)\n","    impix_ymax = int(H * crop_ymax)\n","    return img.crop( (impix_xmin, impix_ymin, impix_xmax, impix_ymax) ), np.array( cropped_labels )\n","\n","def aug_translate(img, labels):\n","    # Compute bounds such that no boxes are cut out\n","    xmin, xmax, ymin, ymax = compute_reasonable_boundary(labels)\n","    trans_range_x = [-xmin, 1 - xmax]\n","    tx = trans_range_x[0] + (np.random.uniform() * (trans_range_x[1] - trans_range_x[0]))\n","    trans_range_y = [-ymin, 1 - ymax]\n","    ty = trans_range_y[0] + (np.random.uniform() * (trans_range_y[1] - trans_range_y[0]))\n","\n","    trans_labels = []\n","    for c,x,y,w,h in labels:\n","        trans_labels.append((c,x+tx,y+ty,w,h))\n","\n","    W,H = img.size\n","    tx_pix = int(W * tx)\n","    ty_pix = int(H * ty)\n","    return img.rotate(0, translate=(tx_pix, ty_pix)), np.array( trans_labels )\n","\n","def aug_colorbalance(img, labels, color_factors=[0.2,2.0]):\n","    factor = color_factors[0] + np.random.uniform() * (color_factors[1] - color_factors[0])\n","    enhancer = ImageEnhance.Color(img)\n","    return enhancer.enhance(factor), labels\n","\n","def aug_contrast(img, labels, contrast_factors=[0.2,2.0]):\n","    factor = contrast_factors[0] + np.random.uniform() * (contrast_factors[1] - contrast_factors[0])\n","    enhancer = ImageEnhance.Contrast(img)\n","    return enhancer.enhance(factor), labels\n","\n","def aug_brightness(img, labels, brightness_factors=[0.2,2.0]):\n","    factor = brightness_factors[0] + np.random.uniform() * (brightness_factors[1] - brightness_factors[0])\n","    enhancer = ImageEnhance.Brightness(img)\n","    return enhancer.enhance(factor), labels\n","\n","def aug_sharpness(img, labels, sharpness_factors=[0.2,10.0]):\n","    factor = sharpness_factors[0] + np.random.uniform() * (sharpness_factors[1] - sharpness_factors[0])\n","    enhancer = ImageEnhance.Sharpness(img)\n","    return enhancer.enhance(factor), labels\n","\n","# Performs no augmentations and returns the original image and bbox. Used for the validation images.\n","def aug_identity(pil_img, label_arr):\n","    return np.array(pil_img), label_arr\n","\n","# This is the default augmentation scheme that we will use for each training image.\n","def aug_default(img, labels, p={'flip':0.5, 'crop':0.5, 'translate':0.5, 'color':0.2, 'contrast':0.2, 'brightness':0.2, 'sharpness':0.2}):\n","    if p['color'] > np.random.uniform():\n","        img, labels = aug_colorbalance(img, labels)\n","    if p['contrast'] > np.random.uniform():\n","        img, labels = aug_contrast(img, labels)\n","    if p['brightness'] > np.random.uniform():\n","        img, labels = aug_brightness(img, labels)\n","    if p['sharpness'] > np.random.uniform():\n","        img, labels = aug_sharpness(img, labels)\n","    if p['flip'] > np.random.uniform():\n","        img, labels = aug_horizontal_flip(img, labels)\n","    if p['crop'] > np.random.uniform():\n","        img, labels = aug_crop(img, labels)\n","    if p['translate'] > np.random.uniform():\n","        img, labels = aug_translate(img, labels)\n","    return np.array(img), labels"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DD-rtvHTI0pS","colab_type":"text"},"source":["## Custom Loss Function"]},{"cell_type":"code","metadata":{"id":"1srd_S6OIlNk","colab_type":"code","colab":{}},"source":["# Shape of ypred: ( batch, i, j, aspect_ratios, 7 ). For a batch,i,j, we get #aspect_ratios vectors of length 7.\n","# Shape of ytrue: ( batch, i, j, aspect_ratios, 9 ). For a batch,i,j, we get #aspect_ratios vectors of length 9 (two more for objectness and cat/loc indicators)\n","def custom_loss(ytrue, ypred):\n","    obj_loss_weight = 1.0\n","    cat_loss_weight = 1.0\n","    loc_loss_weight = 1.0\n","\n","    end_cat = len(cat_list) + 1\n","\n","    objloss_indicators = ytrue[:,:,:,:,-2:-1]\n","    catlocloss_indicators = ytrue[:,:,:,:,-1:]\n","\n","    ytrue_obj, ypred_obj = ytrue[:,:,:,:,:1], ypred[:,:,:,:,:1]\n","    ytrue_obj = tf.where( objloss_indicators != 0, ytrue_obj, 0 )\n","    ypred_obj = tf.where( objloss_indicators != 0, ypred_obj, 0 )\n","    objectness_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)( ytrue_obj, ypred_obj )\n","\n","    ytrue_cat, ypred_cat = ytrue[:,:,:,:,1:end_cat], ypred[:,:,:,:,1:end_cat]\n","    ytrue_cat = tf.where( catlocloss_indicators != 0, ytrue_cat, 0 )\n","    ypred_cat = tf.where( catlocloss_indicators != 0, ypred_cat, 0 )\n","    categorical_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True) ( ytrue_cat, ypred_cat )\n","\n","    # Remember that ytrue is longer than ypred, so we will need to stop at index -2, which is where the indicators are stored\n","    ytrue_loc, ypred_loc = ytrue[:,:,:,:,end_cat:-2], ypred[:,:,:,:,end_cat:]\n","    ytrue_loc = tf.where( catlocloss_indicators != 0, ytrue_loc, 0 )\n","    ypred_loc = tf.where( catlocloss_indicators != 0, ypred_loc, 0 )\n","    localisation_loss = tf.keras.losses.Huber() ( ytrue_loc, ypred_loc )\n","\n","    return obj_loss_weight*objectness_loss + cat_loss_weight*categorical_loss + loc_loss_weight*localisation_loss\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P22FJ9JNI6Lv","colab_type":"text"},"source":["## IOU"]},{"cell_type":"code","metadata":{"id":"yJNl2i-SIn7D","colab_type":"code","colab":{}},"source":["# Computes the intersection-over-union (IoU) of two bounding boxes\n","def iou(bb1, bb2):\n","    x1,y1,w1,h1 = bb1\n","    xmin1 = x1 - w1/2\n","    xmax1 = x1 + w1/2\n","    ymin1 = y1 - h1/2\n","    ymax1 = y1 + h1/2\n","\n","    x2,y2,w2,h2 = bb2\n","    xmin2 = x2 - w2/2\n","    xmax2 = x2 + w2/2\n","    ymin2 = y2 - h2/2\n","    ymax2 = y2 + h2/2\n","\n","    area1 = w1*h1\n","    area2 = w2*h2\n","\n","    # Compute the boundary of the intersection\n","    xmin_int = max(xmin1, xmin2)\n","    xmax_int = min(xmax1, xmax2)\n","    ymin_int = max(ymin1, ymin2)\n","    ymax_int = min(ymax1, ymax2)\n","    intersection = max(xmax_int - xmin_int, 0) * max(ymax_int - ymin_int, 0)\n","\n","    # Remove the double counted region\n","    union = area1+area2-intersection\n","\n","    return intersection / union"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SCf8Z3IzJAeZ","colab_type":"text"},"source":["## Sampling Schemes"]},{"cell_type":"code","metadata":{"id":"98QIHd94IrKL","colab_type":"code","colab":{}},"source":["# Sampling schemes\n","def yolo_posneg_sampling(iou_scores_dict, label_tensor, gtclass, cat_list, iou_threshold=0.5):\n","    iou_scores = []\n","    for _, scores in iou_scores_dict.items():\n","        iou_scores.extend(scores)\n","    iou_scores.sort( key=lambda x: x[0], reverse=True )\n","  \n","    top_iou_score = iou_scores.pop(0)\n","    _, key, i, j, k, dx, dy, dw, dh = top_iou_score\n","    zeros = [0] * len(cat_list)\n","    payload = [1, *zeros, dx,dy,dw,dh]\n","    payload[gtclass + 1] = 1\n","    # Train objectness, class and loc for the positive\n","    label_tensor[key][i,j,k,-2:] = 1\n","    label_tensor[key][i,j,k,:len(payload)] = payload\n","\n","    # Train objectness only for the negatives\n","    low_iou_scores = [iou_score for iou_score in iou_scores if iou_score[0] < iou_threshold]\n","    for _, key, i, j, k, _, _, _, _ in low_iou_scores:\n","        label_tensor[key][i,j,k,-2] = 1\n","\n","def modified_yolo_posneg_sampling(iou_scores_dict, label_tensor, gtclass, cat_list, iou_threshold=0.5):\n","    iou_scores = []\n","    zeros = [0] * len(cat_list)\n","\n","    for _, scores in iou_scores_dict.items():\n","        iou_scores.extend(scores)\n","    iou_scores.sort( key=lambda x: x[0], reverse=True )\n","  \n","    top_iou_score = iou_scores.pop(0)\n","    _, key, i, j, k, dx, dy, dw, dh = top_iou_score\n","    payload = [1, *zeros, dx,dy,dw,dh]\n","    payload[gtclass + 1] = 1\n","    # Train objectness, class and loc for the positive\n","    label_tensor[key][i,j,k,-2:] = 1\n","    label_tensor[key][i,j,k,:len(payload)] = payload\n","\n","    # Train objectness only for the negatives\n","    low_iou_scores = [iou_score for iou_score in iou_scores if iou_score[0] < iou_threshold]\n","    for _, key, i, j, k, _, _, _, _ in low_iou_scores:\n","        label_tensor[key][i,j,k,-2] = 1\n","\n","    # Train cat/loc only for the in-betweens - those with high IoU but not positive\n","    high_iou_scores = [iou_score for iou_score in iou_scores if iou_score[0] >= iou_threshold]\n","    for _, key, i, j, k, dx, dy, dw, dh in high_iou_scores:\n","        label_tensor[key][i,j,k,-1] = 1\n","        payload = [0,*zeros,dx,dy,dw,dh]\n","        payload[gtclass + 1] = 1\n","        label_tensor[key][i,j,k,:len(payload)] = payload\n","\n","def top_ratio_sampling(iou_scores_dict, label_tensor, gtclass, cat_list, positive_ratio=0.25):\n","    iou_scores = []\n","    # Let all tensors learn objectness score\n","    for v in label_tensor.values():\n","        v[:,:,:,-2] = 1\n","  \n","    for _, iou_score_list in iou_scores_dict.items():\n","        iou_score_list.sort( key=lambda x: x[0], reverse=True )\n","        top_percentile_iou_scores = iou_score_list[:round(len(iou_score_list) * positive_ratio)]\n","        # Include the rest that cross the IoU threshold\n","        iou_score_list = top_percentile_iou_scores + [iou_score for iou_score in iou_score_list[len(top_percentile_iou_scores):] if iou_score[0] >= self.iou_threshold]\n","        iou_scores.extend( iou_score_list )\n","\n","    for iou_score in iou_scores:\n","        IoU, key, i, j, k, dx, dy, dw, dh = iou_score\n","        zeros = [0] * len(cat_list)\n","        payload = [IoU, *zeros, dx,dy,dw,dh]\n","        payload[gtclass + 1] = 1\n","        label_tensor[key][i,j,k,:len(payload)] = payload\n","        # Set the classification/localisation indicator at this location to positive\n","        label_tensor[key][i,j,k,-1] = 1\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j48ueWUTJJI2","colab_type":"text"},"source":["## Encoding labels/ Decoding model output"]},{"cell_type":"code","metadata":{"id":"eBO__QLLJKWA","colab_type":"code","colab":{}},"source":["'''\n","Encoder: label -> tensor\n","label_arr: np array like:\n","[[class_idx x y w h]]: num_labels x 5\n","...  \n","Used to figure out for each label line, which tensor entry to shove it into.\n","If the box corresponding to the tensor entry overlaps the ground truth by at least a predefined threshold, then we shove it in.\n","'''\n","\n","def encode_label(label_arr, dims_list, aspect_ratios, iou_fn, sampling_fn, cat_list):\n","    num_entries = 7 + len(cat_list) # objectness, ... len(cat_list) ..., dx, dy, dw, dh, obj_indicator, catloc_indicator\n","    np_labels = {}\n","    for dims in dims_list:\n","        dimkey = '{}x{}'.format(*dims)\n","        np_labels[dimkey] = np.zeros( (*dims, len(aspect_ratios), num_entries ) )\n","\n","    for label in label_arr:\n","        gtclass, gtx, gty, gtw, gth = label\n","        gtclass = int(gtclass)\n","        gt_bbox = [gtx, gty, gtw, gth]\n","    \n","        iou_scores_dict = {}\n","\n","        for dims in dims_list:\n","            key = '{}x{}'.format(*dims)\n","    \n","            kx,ky = dims\n","            gapx = 1.0 / kx\n","            gapy = 1.0 / ky\n","            \n","\n","            # There are kx x ky tiles. \n","            # For now, all have the same w,h of gapx,gapy. \n","            # For the (i,j)-th tile, x = 0.5*gapx + i*gapx = (0.5+i)*gapx | y = (0.5+j)*gapy\n","            \n","            for i in range(kx):\n","                for j in range(ky):\n","                    for k in range( len(aspect_ratios) ):\n","                        dims_aspect_key = (*dims, k) # a 3-tuple: (dim1,dim2,ar)\n","                        if dims_aspect_key not in iou_scores_dict:\n","                            iou_scores_dict[dims_aspect_key] = []\n","                        x = (0.5+i)*gapx\n","                        y = (0.5+j)*gapy\n","\n","                        # Different aspect ratios alter the anchor box default dimensions\n","                        w = gapx * aspect_ratios[k][0]\n","                        h = gapy * aspect_ratios[k][1]\n","                        cand_bbox = [x,y,w,h]\n","\n","                        # SSD formulation\n","                        dx = (gtx - x) / w \n","                        dy = (gty - y) / h\n","                        dw = log( gtw / w )\n","                        dh = log( gth / h )\n","            \n","                        int_over_union = iou_fn( cand_bbox, gt_bbox )\n","                        iou_scores_dict[dims_aspect_key].append( (int_over_union, key, i, j, k, dx, dy, dw, dh) )\n","            sampling_fn( iou_scores_dict, np_labels, gtclass, cat_list )\n","    return np_labels\n","\n","def decode_tensor(pred_dict, aspect_ratios):\n","    results = []\n","    for dim_str, pred_tensor in pred_dict.items():\n","        pred_tensor = pred_tensor[0] # remove the batch\n","        kx, ky = [int(g) for g in dim_str.split('x')]\n","        gapx = 1. / kx\n","        gapy = 1. / ky\n","\n","        # We trained without activations, so we need to process the logits into probabilities/scores\n","        pred_arr = np.array(pred_tensor)\n","        obj_logits = pred_arr[:,:,:,0]\n","        obj_scores = 1. / (1 + np.exp(-obj_logits))\n","        pred_arr[:,:,:,0] = obj_scores\n","\n","        cls_logits = pred_arr[:,:,:,1:-4]\n","        cls_scores = np.exp(cls_logits)\n","        cls_scores = cls_scores / cls_scores.sum(axis=-1)[...,np.newaxis]\n","        pred_arr[:,:,:,1:-4] = cls_scores\n","\n","        for k, ar in enumerate(aspect_ratios):\n","            for i in range(kx):\n","                for j in range(ky):\n","                    cx = (0.5+i)*gapx\n","                    cy = (0.5+j)*gapy\n","                    w = gapx * ar[0]\n","                    h = gapy * ar[1]\n","\n","                    payload = pred_arr[i,j,k]\n","                    obj_score = payload[0]\n","                    dx, dy, dw, dh = payload[-4:]\n","                    cls_probs = payload[1:-4]\n","\n","                    predx = (dx * w) + cx\n","                    predy = (dy * h) + cy\n","                    predw = w * exp( dw )\n","                    predh = h * exp( dh )\n","                    max_cls_idx = np.argmax( cls_probs )\n","                    max_cls_prob = cls_probs[max_cls_idx]\n","                    category_id = max_cls_idx + 1\n","                    det_score = obj_score * max_cls_prob\n","                    results.append( (det_score, category_id, predx, predy, predw, predh) )\n","    return results"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IhMy4TlYJW4j","colab_type":"text"},"source":["## Data Generators"]},{"cell_type":"code","metadata":{"id":"CZ_snVQlJROm","colab_type":"code","colab":{}},"source":["class TILSequence(Sequence):\n","\n","    def __init__(self, img_folder, json_annotation_file, batch_size, augment_fn, input_size, label_encoder, preprocess_fn, testmode=False):\n","        self._prepare_data(img_folder, json_annotation_file)\n","        self.batch_size = batch_size\n","        self.augment_fn = augment_fn\n","        self.input_wh = (*input_size[:2][::-1],input_size[2])\n","        self.label_encoder = label_encoder\n","        self.preprocess_fn = preprocess_fn\n","        self.testmode = testmode\n","    \n","    def _prepare_data(self, img_folder, json_annotation_file):\n","        imgs_dict = {im.split('.')[0]:im for im in os.listdir(img_folder) if im.endswith('.jpg')}\n","        data_dict = {}\n","        with open(json_annotation_file, 'r') as f:\n","            annotations_dict = json.load(f)\n","        annotations_list = annotations_dict['annotations']\n","        for annotation in annotations_list:\n","            img_id = str(annotation['image_id'])\n","            c = annotation['category_id'] - 1 # TODO: make sure that category ids start from 1, not 0\n","            boxleft,boxtop,boxwidth,boxheight = annotation['bbox']\n","            if img_id in imgs_dict:\n","                img_fp = os.path.join(img_folder, imgs_dict[img_id])\n","                imwidth,imheight = PIL.Image.open(img_fp).size\n","                if img_id not in data_dict:\n","                    data_dict[img_id] = []\n","                box_cenx = boxleft + boxwidth/2.\n","                box_ceny = boxtop + boxheight/2.\n","                x,y,w,h = box_cenx/imwidth, box_ceny/imheight, boxwidth/imwidth, boxheight/imheight\n","\n","                data_dict[img_id].append( [c,x,y,w,h] )\n","        self.x, self.y, self.ids = [], [], []\n","        for img_id, labels in data_dict.items():\n","            self.x.append( os.path.join(img_folder, imgs_dict[img_id]) )\n","            self.y.append( np.array(labels) )\n","            self.ids.append( img_id )\n","\n","    def __len__(self):\n","        return int(np.ceil(len(self.x) / float(self.batch_size)))\n","  \n","    def __getitem__(self, idx):\n","        return self.get_batch_test(idx) if self.testmode else self.get_batch(idx)\n","\n","    def get_batch_test(self, idx):\n","        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n","        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n","        batch_ids = self.ids[idx * self.batch_size:(idx + 1) * self.batch_size]\n","\n","        x_acc, y_acc = [], {}\n","        original_img_dims = []\n","        with Pool(self.batch_size) as p:\n","            # Read in the PIL objects from filepaths\n","            batch_x = p.map(load_img, batch_x)\n","    \n","        for x,y in zip( batch_x, batch_y ):\n","            W,H = x.size\n","            original_img_dims.append( (W,H) )\n","            x_aug, y_aug = self.augment_fn( x, y )\n","            if x_aug.size != self.input_wh[:2]:\n","                x_aug.resize( self.input_wh )\n","            x_acc.append( np.array(x_aug) )\n","            y_dict = self.label_encoder( y_aug )\n","            for dimkey, label in y_dict.items():\n","                if dimkey not in y_acc:\n","                    y_acc[dimkey] = []\n","                y_acc[dimkey].append( label )\n","\n","        return batch_ids, original_img_dims, self.preprocess_fn( np.array( x_acc ) ), { dimkey: np.array( gt_tensor ) for dimkey, gt_tensor in y_acc.items() }\n","\n","    def get_batch(self, idx):\n","        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n","        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n","\n","        x_acc, y_acc = [], {}\n","        with Pool(self.batch_size) as p:\n","            # Read in the PIL objects from filepaths\n","            batch_x = p.map(load_img, batch_x)\n","\n","        for x,y in zip( batch_x, batch_y ):\n","            x_aug, y_aug = self.augment_fn( x, y )\n","            if x_aug.size != self.input_wh[:2]:\n","                x_aug.resize( self.input_wh )\n","            x_acc.append( np.array(x_aug) )\n","            y_dict = self.label_encoder( y_aug )\n","            for dimkey, label in y_dict.items():\n","                if dimkey not in y_acc:\n","                    y_acc[dimkey] = []\n","                y_acc[dimkey].append( label )\n","\n","        return self.preprocess_fn( np.array( x_acc ) ), { dimkey: np.array( gt_tensor ) for dimkey, gt_tensor in y_acc.items() }\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7_s5AA9mJZui","colab_type":"code","colab":{}},"source":["class TILPickle(Sequence):\n","    def __init__(self, pickle_file, batch_size, augment_fn, input_size, label_encoder, preprocess_fn, testmode=False):\n","        with open(pickle_file, 'rb') as p:\n","            self.ids, self.x, self.y = pickle.load(p)\n","        self.batch_size = batch_size\n","        self.augment_fn = augment_fn\n","        self.input_wh = (*input_size[:2][::-1],input_size[2])\n","        self.label_encoder = label_encoder\n","        self.preprocess_fn = preprocess_fn\n","        self.testmode = testmode\n","    \n","    def __len__(self):\n","        return int(np.ceil(len(self.x) / float(self.batch_size)))\n","  \n","    def __getitem__(self, idx):\n","        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n","        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n","        batch_ids = self.ids[idx * self.batch_size:(idx + 1) * self.batch_size]\n","\n","        x_acc, y_acc = [], {}\n","    \n","        for x,y in zip( batch_x, batch_y ):\n","            x_aug, y_aug = self.augment_fn( x, y )\n","            if x_aug.size != self.input_wh[:2]:\n","                x_aug.resize( self.input_wh )\n","            x_acc.append( np.array(x_aug) )\n","            y_dict = self.label_encoder( y_aug )\n","            for dimkey, label in y_dict.items():\n","                if dimkey not in y_acc:\n","                    y_acc[dimkey] = []\n","                y_acc[dimkey].append(label)\n","\n","        if self.testmode:\n","            return batch_ids, self.preprocess_fn( np.array( x_acc ) ), { dimkey: np.array( gt_tensor ) for dimkey, gt_tensor in y_acc.items() }\n","        return self.preprocess_fn( np.array( x_acc ) ), { dimkey: np.array( gt_tensor ) for dimkey, gt_tensor in y_acc.items() }"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6wxIQJBBJ-0u","colab_type":"text"},"source":["## Constructing Models"]},{"cell_type":"code","metadata":{"id":"fM0J24l-JcAg","colab_type":"code","colab":{}},"source":["dims_list = [(7,7)]\n","def transfer_model(backbone_model, input_shape, dims_list, num_aspect_ratios, wt_decay, model_name='transfer-objdet-model'):\n","    inputs = keras.Input(shape=input_shape)\n","    backbone_output = backbone_model(inputs) #7\n","\n","    x = layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay))(backbone_output) #7\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU(0.01)(x)\n","    x = layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #5\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU(0.01)(x)\n","    x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #5\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU(0.01)(x)\n","    x = layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #3\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU(0.01)(x)\n","\n","    x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #3\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU(0.01)(x)\n","    x = layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #3\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU(0.01)(x)\n","\n","    # You can accumulate more scales via shortcut. Imagine each (n,m) is a grid super-imposed on the original image.\n","    # See the next cell for an example for more scales.\n","    dim_tensor_map = {'7x7': x}\n","\n","    # For each dimension, construct a predictions tensor. Accumulate them into a dictionary for keras to understand multiple labels.\n","    preds_dict = {}\n","    for dims in dims_list:\n","        dimkey = '{}x{}'.format(*dims)\n","        tens = dim_tensor_map[dimkey]\n","        ar_preds = []\n","        for _ in range(num_aspect_ratios):\n","            objectness_preds = layers.Conv2D(1, 1, kernel_regularizer=l2(wt_decay))( tens )\n","            class_preds = layers.Conv2D(len(cat_list), 1, kernel_regularizer=l2(wt_decay))( tens )\n","            bbox_preds = layers.Conv2D(4, 1, kernel_regularizer=l2(wt_decay))( tens )\n","            ar_preds.append( layers.Concatenate()([objectness_preds, class_preds, bbox_preds]) )\n","\n","        if num_aspect_ratios > 1:\n","            predictions = layers.Concatenate()(ar_preds)\n","        elif num_aspect_ratios == 1:\n","            predictions = ar_preds[0]\n","    \n","        predictions = layers.Reshape( (*dims, num_aspect_ratios, 5+len(cat_list)), name=dimkey )(predictions)\n","        preds_dict[dimkey] = predictions\n","\n","    model = keras.Model(inputs, preds_dict, name=model_name)\n","\n","    model.compile( optimizer=tf.keras.optimizers.Adam(1e-5),\n","                 loss=custom_loss )\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cbA9GtEgJ_c6","colab_type":"code","colab":{}},"source":["def transfer_model_7x7_14x14(backbone_model, input_shape, dims_list, num_aspect_ratios, wt_decay, model_name='transfer-objdet-model-7x7-14x14'):\n","    inputs = keras.Input(shape=input_shape)\n","    intermediate_layer_model = keras.Model(inputs=backbone_model.input,\n","                                         outputs=backbone_model.get_layer('conv4_block6_out').output)\n","    intermediate_output = intermediate_layer_model(inputs) #14\n","    backbone_output = backbone_model(inputs) #7\n","\n","    x = layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay))(backbone_output) #7\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU(0.01)(x)\n","    x = layers.Conv2D(1024, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #7\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU(0.01)(x)\n","    x = layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #7\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU(0.01)(x)\n","    x = layers.Conv2D(1024, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #7\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU(0.01)(x)\n","    x = layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #7\n","    x = layers.BatchNormalization()(x)\n","    upsample = layers.LeakyReLU(0.01)(x)\n","    x = layers.Conv2D(2048, 3, padding='same', kernel_regularizer=l2(wt_decay))(upsample) #7\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU(0.01)(x)\n","    tens_7x7 = layers.Add()([x,backbone_output])\n","\n","    x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(upsample) #7\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU(0.01)(x)\n","    x = layers.Conv2DTranspose(512, 5, strides=(2, 2), padding='same')(x) #14\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU(0.01)(x)\n","\n","    x = layers.Concatenate()([x,intermediate_output])\n","\n","    x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU(0.01)(x)\n","    x = layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU(0.01)(x)\n","    x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU(0.01)(x)\n","    x = layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU(0.01)(x)\n","    x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU(0.01)(x)\n","    x = layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n","    x = layers.BatchNormalization()(x)\n","    tens_14x14 = layers.LeakyReLU(0.01)(x)\n","\n","    dim_tensor_map = {'7x7': tens_7x7, '14x14': tens_14x14}\n","\n","    # For each dimension, construct a predictions tensor. Accumulate them into a dictionary for keras to understand multiple labels.\n","    preds_dict = {}\n","    for dims in dims_list:\n","        dimkey = '{}x{}'.format(*dims)\n","        tens = dim_tensor_map[dimkey]\n","        ar_preds = []\n","        for _ in range(num_aspect_ratios):\n","            objectness_preds = layers.Conv2D(1, 1, kernel_regularizer=l2(wt_decay))( tens )\n","            class_preds = layers.Conv2D(len(cat_list), 1, kernel_regularizer=l2(wt_decay))( tens )\n","            bbox_preds = layers.Conv2D(4, 1, kernel_regularizer=l2(wt_decay))( tens )\n","            ar_preds.append( layers.Concatenate()([objectness_preds, class_preds, bbox_preds]) )\n","\n","        if num_aspect_ratios > 1:\n","            predictions = layers.Concatenate()(ar_preds)\n","        elif num_aspect_ratios == 1:\n","            predictions = ar_preds[0]\n","    \n","        predictions = layers.Reshape( (*dims, num_aspect_ratios, 5+len(cat_list)), name=dimkey )(predictions)\n","        preds_dict[dimkey] = predictions\n","\n","    model = keras.Model(inputs, preds_dict, name=model_name)\n","\n","    model.compile( optimizer=tf.keras.optimizers.Adam(1e-5),\n","                 loss=custom_loss )\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sut7VnL2KEVK","colab_type":"code","outputId":"fc1cfa93-6d5a-437a-de4c-002266825175","executionInfo":{"status":"ok","timestamp":1592322381844,"user_tz":-480,"elapsed":31698,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"17512883430045220190"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Choose whether to start a new model \n","# or load a previously trained one\n","model_context = 'model-7x7-14x14-3aspect-modyoloposneg-wd{}'.format(wt_decay)\n","\n","#load_model_path = os.path.join( load_model_folder, '{}-best_val_loss.h5'.format(model_context) )\n","load_model_path = None\n","\n","if load_model_path is None:\n","    #backbone_model = tf.keras.applications.ResNet50(input_shape=input_shape, include_top=False)\n","    backbone_model = tf.keras.applications.InceptionResNetV2(input_shape=input_shape, include_top=False)\n","    #model = transfer_model_7x7_14x14(backbone_model, input_shape=input_shape, dims_list=dims_list, num_aspect_ratios=len(aspect_ratios), wt_decay=wt_decay, model_name=model_context+'-res50')\n","    model = transfer_model(backbone_model, input_shape=input_shape, dims_list=dims_list, num_aspect_ratios=len(aspect_ratios), wt_decay=wt_decay, model_name=model_context)\n","else:\n","    model = tf.keras.models.load_model(load_model_path, custom_objects={'custom_loss':custom_loss})\n","model.summary()"],"execution_count":66,"outputs":[{"output_type":"stream","text":["Model: \"model-7x7-14x14-3aspect-modyoloposneg-wd0.0005\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_14 (InputLayer)           [(None, 224, 224, 3) 0                                            \n","__________________________________________________________________________________________________\n","inception_resnet_v2 (Model)     (None, 5, 5, 1536)   54336736    input_14[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_893 (Conv2D)             (None, 5, 5, 512)    786944      inception_resnet_v2[1][0]        \n","__________________________________________________________________________________________________\n","batch_normalization_848 (BatchN (None, 5, 5, 512)    2048        conv2d_893[0][0]                 \n","__________________________________________________________________________________________________\n","leaky_re_lu_36 (LeakyReLU)      (None, 5, 5, 512)    0           batch_normalization_848[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_894 (Conv2D)             (None, 5, 5, 512)    2359808     leaky_re_lu_36[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_849 (BatchN (None, 5, 5, 512)    2048        conv2d_894[0][0]                 \n","__________________________________________________________________________________________________\n","leaky_re_lu_37 (LeakyReLU)      (None, 5, 5, 512)    0           batch_normalization_849[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_895 (Conv2D)             (None, 5, 5, 256)    131328      leaky_re_lu_37[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_850 (BatchN (None, 5, 5, 256)    1024        conv2d_895[0][0]                 \n","__________________________________________________________________________________________________\n","leaky_re_lu_38 (LeakyReLU)      (None, 5, 5, 256)    0           batch_normalization_850[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_896 (Conv2D)             (None, 5, 5, 512)    1180160     leaky_re_lu_38[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_851 (BatchN (None, 5, 5, 512)    2048        conv2d_896[0][0]                 \n","__________________________________________________________________________________________________\n","leaky_re_lu_39 (LeakyReLU)      (None, 5, 5, 512)    0           batch_normalization_851[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_897 (Conv2D)             (None, 5, 5, 256)    131328      leaky_re_lu_39[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_852 (BatchN (None, 5, 5, 256)    1024        conv2d_897[0][0]                 \n","__________________________________________________________________________________________________\n","leaky_re_lu_40 (LeakyReLU)      (None, 5, 5, 256)    0           batch_normalization_852[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_898 (Conv2D)             (None, 5, 5, 512)    1180160     leaky_re_lu_40[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_853 (BatchN (None, 5, 5, 512)    2048        conv2d_898[0][0]                 \n","__________________________________________________________________________________________________\n","leaky_re_lu_41 (LeakyReLU)      (None, 5, 5, 512)    0           batch_normalization_853[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_899 (Conv2D)             (None, 5, 5, 1)      513         leaky_re_lu_41[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_900 (Conv2D)             (None, 5, 5, 5)      2565        leaky_re_lu_41[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_901 (Conv2D)             (None, 5, 5, 4)      2052        leaky_re_lu_41[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_902 (Conv2D)             (None, 5, 5, 1)      513         leaky_re_lu_41[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_903 (Conv2D)             (None, 5, 5, 5)      2565        leaky_re_lu_41[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_904 (Conv2D)             (None, 5, 5, 4)      2052        leaky_re_lu_41[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_905 (Conv2D)             (None, 5, 5, 1)      513         leaky_re_lu_41[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_906 (Conv2D)             (None, 5, 5, 5)      2565        leaky_re_lu_41[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_907 (Conv2D)             (None, 5, 5, 4)      2052        leaky_re_lu_41[0][0]             \n","__________________________________________________________________________________________________\n","concatenate_20 (Concatenate)    (None, 5, 5, 10)     0           conv2d_899[0][0]                 \n","                                                                 conv2d_900[0][0]                 \n","                                                                 conv2d_901[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_21 (Concatenate)    (None, 5, 5, 10)     0           conv2d_902[0][0]                 \n","                                                                 conv2d_903[0][0]                 \n","                                                                 conv2d_904[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_22 (Concatenate)    (None, 5, 5, 10)     0           conv2d_905[0][0]                 \n","                                                                 conv2d_906[0][0]                 \n","                                                                 conv2d_907[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_23 (Concatenate)    (None, 5, 5, 30)     0           concatenate_20[0][0]             \n","                                                                 concatenate_21[0][0]             \n","                                                                 concatenate_22[0][0]             \n","__________________________________________________________________________________________________\n","7x7 (Reshape)                   (None, 7, 7, 3, 10)  0           concatenate_23[0][0]             \n","==================================================================================================\n","Total params: 60,132,094\n","Trainable params: 60,066,430\n","Non-trainable params: 65,664\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B-8zEFSUKWhq","colab_type":"text"},"source":["## Training/Transfer Learning of Model"]},{"cell_type":"code","metadata":{"id":"4W9qNyKBKIFQ","colab_type":"code","outputId":"4ae8d75c-9ae5-4832-c8a5-cce9ca43b18e","executionInfo":{"status":"ok","timestamp":1592322423268,"user_tz":-480,"elapsed":26574,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"17512883430045220190"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["'''\n","- There is overfitting now that I set top 25% (of each dim-ar combination) as positives. How?\n","- Larger image size - maybe 448\n","- Transfer learning\n","- Change weights of losses?\n","\n","# Also add more callbacks, such as tensorboard \n","dataset, batch_size, augment_fn, input_size, label_encoder, preprocess_fn\n","encode_label(label_arr, dims_list, aspect_ratios, iou_fn, sampling_fn, cat_list)\n","img_folder, json_annotation_file, batch_size, augment_fn, input_size, label_encoder, preprocess_fn\n","'''\n","\n","bs=16\n","n_epochs_warmup = 30\n","n_epochs_after = 80\n","\n","label_encoder = lambda y: encode_label(y, dims_list, aspect_ratios, iou, modified_yolo_posneg_sampling, cat_list)\n","preproc_fn = lambda x: x / 255.\n","\n","print('Creating training sequence...')\n","#train_sequence = TILSequence(train_imgs_folder, train_annotations, bs, aug_default, input_shape, label_encoder, preproc_fn)\n","train_sequence = TILPickle(train_pickle, bs, aug_default, input_shape, label_encoder, preproc_fn)\n","print('Creating validation sequence...')\n","#val_sequence = TILSequence(val_imgs_folder, val_annotations, bs, aug_identity, input_shape, label_encoder, preproc_fn)\n","val_sequence = TILPickle(val_pickle, bs, aug_identity, input_shape, label_encoder, preproc_fn)"],"execution_count":67,"outputs":[{"output_type":"stream","text":["Creating training sequence...\n","Creating validation sequence...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SRBSZy-iKXmh","colab_type":"code","colab":{}},"source":["save_model_path = os.path.join(save_model_folder, '{}-best_val_loss.h5'.format(model_context))\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","                                                                filepath=save_model_path,\n","                                                                save_weights_only=False,\n","                                                                monitor='val_loss',\n","                                                                mode='auto',\n","                                                                save_best_only=True)\n","\n","earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-8)\n","\n","for layer in backbone_model.layers:\n","    layer.trainable = False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ykhJC9EKi9R","colab_type":"code","outputId":"28019c74-a345-4b2a-8a9a-50bc632ff870","executionInfo":{"status":"error","timestamp":1592322525330,"user_tz":-480,"elapsed":33576,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"17512883430045220190"}},"colab":{"base_uri":"https://localhost:8080/","height":498}},"source":["print('Warming up the model...')\n","model.fit(x=train_sequence,  \n","          epochs=n_epochs_warmup, \n","          validation_data=val_sequence,\n","          callbacks=[model_checkpoint_callback, earlystopping, reduce_lr])"],"execution_count":69,"outputs":[{"output_type":"stream","text":["Warming up the model...\n","Epoch 1/30\n"],"name":"stdout"},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-69-c044417dba4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs_warmup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sequence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           callbacks=[model_checkpoint_callback, earlystopping, reduce_lr])\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m:  Input to reshape is a tensor with 12000 values, but the requested shape has 23520\n\t [[node model-7x7-14x14-3aspect-modyoloposneg-wd0.0005/7x7/Reshape (defined at <ipython-input-69-c044417dba4b>:5) ]] [Op:__inference_train_function_321506]\n\nFunction call stack:\ntrain_function\n"]}]},{"cell_type":"code","metadata":{"id":"v8R3xiNAKlSX","colab_type":"code","outputId":"3563781b-5443-4bb2-c45c-63ccdb2e47b9","executionInfo":{"status":"ok","timestamp":1592212366024,"user_tz":-480,"elapsed":11062009,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"17512883430045220190"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Fine tuning\n","print('Model warmed. Loading best val version of model...')\n","load_model_path = os.path.join( load_model_folder, '{}-best_val_loss.h5'.format(model_context) )\n","del model\n","model = tf.keras.models.load_model(load_model_path, custom_objects={'custom_loss':custom_loss})\n","\n","for layer in model.get_layer('resnet50').layers:\n","    layer.trainable = True\n","\n","model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss=custom_loss)\n","model_context = 'ft-' + model_context\n","save_model_path = os.path.join( save_model_folder, '{}-best_val_loss.h5'.format(model_context) )\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","                                                                filepath=save_model_path,\n","                                                                save_weights_only=False,\n","                                                                monitor='val_loss',\n","                                                                mode='auto',\n","                                                                save_best_only=True)\n","earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-8)\n","\n","model.fit(x=train_sequence, \n","          epochs=n_epochs_after, \n","          validation_data=val_sequence, \n","          callbacks=[model_checkpoint_callback, earlystopping, reduce_lr])\n","\n","# Final save\n","model.save(os.path.join(save_model_folder, '{}-final.h5'.format(model_context)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model warmed. Loading best val version of model...\n","WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n","Epoch 1/70\n","515/515 [==============================] - 160s 310ms/step - loss: 0.1154 - 14x14_loss: 0.0012 - 7x7_loss: 0.0435 - val_loss: 0.1086 - val_14x14_loss: 0.0013 - val_7x7_loss: 0.0401 - lr: 1.0000e-05\n","Epoch 2/70\n","515/515 [==============================] - 163s 317ms/step - loss: 0.1086 - 14x14_loss: 0.0012 - 7x7_loss: 0.0432 - val_loss: 0.1014 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0388 - lr: 1.0000e-05\n","Epoch 3/70\n","515/515 [==============================] - 162s 314ms/step - loss: 0.1028 - 14x14_loss: 0.0012 - 7x7_loss: 0.0427 - val_loss: 0.0963 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0388 - lr: 1.0000e-05\n","Epoch 4/70\n","515/515 [==============================] - 162s 314ms/step - loss: 0.0976 - 14x14_loss: 0.0012 - 7x7_loss: 0.0423 - val_loss: 0.0926 - val_14x14_loss: 0.0013 - val_7x7_loss: 0.0395 - lr: 1.0000e-05\n","Epoch 5/70\n","515/515 [==============================] - 161s 313ms/step - loss: 0.0937 - 14x14_loss: 0.0012 - 7x7_loss: 0.0426 - val_loss: 0.0886 - val_14x14_loss: 0.0013 - val_7x7_loss: 0.0394 - lr: 1.0000e-05\n","Epoch 6/70\n","515/515 [==============================] - 161s 314ms/step - loss: 0.0896 - 14x14_loss: 0.0012 - 7x7_loss: 0.0423 - val_loss: 0.0852 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0395 - lr: 1.0000e-05\n","Epoch 7/70\n","515/515 [==============================] - 161s 313ms/step - loss: 0.0856 - 14x14_loss: 0.0012 - 7x7_loss: 0.0414 - val_loss: 0.0806 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0379 - lr: 1.0000e-05\n","Epoch 8/70\n","515/515 [==============================] - 161s 313ms/step - loss: 0.0824 - 14x14_loss: 0.0012 - 7x7_loss: 0.0412 - val_loss: 0.0789 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0389 - lr: 1.0000e-05\n","Epoch 9/70\n","515/515 [==============================] - 162s 314ms/step - loss: 0.0793 - 14x14_loss: 0.0012 - 7x7_loss: 0.0407 - val_loss: 0.0759 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0386 - lr: 1.0000e-05\n","Epoch 10/70\n","515/515 [==============================] - 162s 314ms/step - loss: 0.0770 - 14x14_loss: 0.0011 - 7x7_loss: 0.0409 - val_loss: 0.0735 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0384 - lr: 1.0000e-05\n","Epoch 11/70\n","515/515 [==============================] - 161s 312ms/step - loss: 0.0745 - 14x14_loss: 0.0012 - 7x7_loss: 0.0405 - val_loss: 0.0719 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0389 - lr: 1.0000e-05\n","Epoch 12/70\n","515/515 [==============================] - 163s 316ms/step - loss: 0.0724 - 14x14_loss: 0.0012 - 7x7_loss: 0.0404 - val_loss: 0.0714 - val_14x14_loss: 0.0020 - val_7x7_loss: 0.0395 - lr: 1.0000e-05\n","Epoch 13/70\n","515/515 [==============================] - 161s 313ms/step - loss: 0.0697 - 14x14_loss: 0.0011 - 7x7_loss: 0.0396 - val_loss: 0.0674 - val_14x14_loss: 0.0013 - val_7x7_loss: 0.0380 - lr: 1.0000e-05\n","Epoch 14/70\n","515/515 [==============================] - 161s 313ms/step - loss: 0.0682 - 14x14_loss: 0.0012 - 7x7_loss: 0.0398 - val_loss: 0.0659 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0384 - lr: 1.0000e-05\n","Epoch 15/70\n","515/515 [==============================] - 161s 313ms/step - loss: 0.0666 - 14x14_loss: 0.0011 - 7x7_loss: 0.0397 - val_loss: 0.0647 - val_14x14_loss: 0.0013 - val_7x7_loss: 0.0384 - lr: 1.0000e-05\n","Epoch 16/70\n","515/515 [==============================] - 161s 312ms/step - loss: 0.0650 - 14x14_loss: 0.0011 - 7x7_loss: 0.0395 - val_loss: 0.0632 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0383 - lr: 1.0000e-05\n","Epoch 17/70\n","515/515 [==============================] - 165s 320ms/step - loss: 0.0637 - 14x14_loss: 0.0011 - 7x7_loss: 0.0394 - val_loss: 0.0626 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0389 - lr: 1.0000e-05\n","Epoch 18/70\n","515/515 [==============================] - 165s 320ms/step - loss: 0.0626 - 14x14_loss: 0.0011 - 7x7_loss: 0.0395 - val_loss: 0.0605 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0379 - lr: 1.0000e-05\n","Epoch 19/70\n","515/515 [==============================] - 164s 318ms/step - loss: 0.0612 - 14x14_loss: 0.0011 - 7x7_loss: 0.0391 - val_loss: 0.0585 - val_14x14_loss: 0.0013 - val_7x7_loss: 0.0367 - lr: 1.0000e-05\n","Epoch 20/70\n","515/515 [==============================] - 161s 313ms/step - loss: 0.0602 - 14x14_loss: 0.0011 - 7x7_loss: 0.0391 - val_loss: 0.0583 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0376 - lr: 1.0000e-05\n","Epoch 21/70\n","515/515 [==============================] - 154s 300ms/step - loss: 0.0589 - 14x14_loss: 0.0011 - 7x7_loss: 0.0387 - val_loss: 0.0593 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0394 - lr: 1.0000e-05\n","Epoch 22/70\n","515/515 [==============================] - 161s 312ms/step - loss: 0.0576 - 14x14_loss: 0.0011 - 7x7_loss: 0.0383 - val_loss: 0.0569 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0379 - lr: 1.0000e-05\n","Epoch 23/70\n","515/515 [==============================] - 161s 313ms/step - loss: 0.0567 - 14x14_loss: 0.0011 - 7x7_loss: 0.0381 - val_loss: 0.0556 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0373 - lr: 1.0000e-05\n","Epoch 24/70\n","515/515 [==============================] - 160s 311ms/step - loss: 0.0558 - 14x14_loss: 0.0011 - 7x7_loss: 0.0379 - val_loss: 0.0543 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0368 - lr: 1.0000e-05\n","Epoch 25/70\n","515/515 [==============================] - 153s 298ms/step - loss: 0.0551 - 14x14_loss: 0.0011 - 7x7_loss: 0.0380 - val_loss: 0.0822 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0652 - lr: 1.0000e-05\n","Epoch 26/70\n","515/515 [==============================] - 160s 310ms/step - loss: 0.0545 - 14x14_loss: 0.0011 - 7x7_loss: 0.0379 - val_loss: 0.0541 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0377 - lr: 1.0000e-05\n","Epoch 27/70\n","515/515 [==============================] - 154s 298ms/step - loss: 0.0537 - 14x14_loss: 0.0011 - 7x7_loss: 0.0377 - val_loss: 0.0547 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0388 - lr: 1.0000e-05\n","Epoch 28/70\n","515/515 [==============================] - 160s 311ms/step - loss: 0.0531 - 14x14_loss: 0.0011 - 7x7_loss: 0.0375 - val_loss: 0.0534 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0380 - lr: 1.0000e-05\n","Epoch 29/70\n","515/515 [==============================] - 161s 313ms/step - loss: 0.0526 - 14x14_loss: 0.0011 - 7x7_loss: 0.0376 - val_loss: 0.0528 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0379 - lr: 1.0000e-05\n","Epoch 30/70\n","515/515 [==============================] - 154s 299ms/step - loss: 0.0517 - 14x14_loss: 0.0010 - 7x7_loss: 0.0371 - val_loss: 0.0541 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0396 - lr: 1.0000e-05\n","Epoch 31/70\n","515/515 [==============================] - 160s 312ms/step - loss: 0.0512 - 14x14_loss: 0.0011 - 7x7_loss: 0.0370 - val_loss: 0.0519 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0379 - lr: 1.0000e-05\n","Epoch 32/70\n","515/515 [==============================] - 154s 298ms/step - loss: 0.0508 - 14x14_loss: 0.0011 - 7x7_loss: 0.0371 - val_loss: 0.0524 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0387 - lr: 1.0000e-05\n","Epoch 33/70\n","515/515 [==============================] - 160s 312ms/step - loss: 0.0501 - 14x14_loss: 0.0010 - 7x7_loss: 0.0367 - val_loss: 0.0507 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0373 - lr: 1.0000e-05\n","Epoch 34/70\n","515/515 [==============================] - 154s 298ms/step - loss: 0.0498 - 14x14_loss: 0.0010 - 7x7_loss: 0.0367 - val_loss: 0.0511 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0382 - lr: 1.0000e-05\n","Epoch 35/70\n","515/515 [==============================] - 153s 297ms/step - loss: 0.0493 - 14x14_loss: 0.0011 - 7x7_loss: 0.0366 - val_loss: 0.0509 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0382 - lr: 1.0000e-05\n","Epoch 36/70\n","515/515 [==============================] - 152s 295ms/step - loss: 0.0488 - 14x14_loss: 0.0010 - 7x7_loss: 0.0364 - val_loss: 0.0507 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0383 - lr: 1.0000e-05\n","Epoch 37/70\n","515/515 [==============================] - 159s 310ms/step - loss: 0.0485 - 14x14_loss: 0.0011 - 7x7_loss: 0.0363 - val_loss: 0.0491 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0370 - lr: 1.0000e-05\n","Epoch 38/70\n","515/515 [==============================] - 153s 298ms/step - loss: 0.0482 - 14x14_loss: 0.0010 - 7x7_loss: 0.0363 - val_loss: 0.0495 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0376 - lr: 1.0000e-05\n","Epoch 39/70\n","515/515 [==============================] - 162s 315ms/step - loss: 0.0475 - 14x14_loss: 0.0010 - 7x7_loss: 0.0358 - val_loss: 0.0484 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0368 - lr: 1.0000e-05\n","Epoch 40/70\n","515/515 [==============================] - 154s 298ms/step - loss: 0.0473 - 14x14_loss: 0.0010 - 7x7_loss: 0.0359 - val_loss: 0.0494 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0380 - lr: 1.0000e-05\n","Epoch 41/70\n","515/515 [==============================] - 152s 295ms/step - loss: 0.0468 - 14x14_loss: 0.0010 - 7x7_loss: 0.0357 - val_loss: 0.0496 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0384 - lr: 1.0000e-05\n","Epoch 42/70\n","515/515 [==============================] - 152s 295ms/step - loss: 0.0471 - 14x14_loss: 0.0010 - 7x7_loss: 0.0362 - val_loss: 0.0495 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0385 - lr: 1.0000e-05\n","Epoch 43/70\n","515/515 [==============================] - 152s 295ms/step - loss: 0.0464 - 14x14_loss: 0.0010 - 7x7_loss: 0.0356 - val_loss: 0.0492 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0383 - lr: 1.0000e-05\n","Epoch 44/70\n","515/515 [==============================] - 152s 295ms/step - loss: 0.0463 - 14x14_loss: 0.0011 - 7x7_loss: 0.0357 - val_loss: 0.0498 - val_14x14_loss: 0.0013 - val_7x7_loss: 0.0391 - lr: 1.0000e-05\n","Epoch 45/70\n","515/515 [==============================] - 160s 310ms/step - loss: 0.0443 - 14x14_loss: 9.8605e-04 - 7x7_loss: 0.0339 - val_loss: 0.0478 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0373 - lr: 2.0000e-06\n","Epoch 46/70\n","515/515 [==============================] - 160s 312ms/step - loss: 0.0439 - 14x14_loss: 9.7719e-04 - 7x7_loss: 0.0336 - val_loss: 0.0476 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0373 - lr: 2.0000e-06\n","Epoch 47/70\n","515/515 [==============================] - 153s 297ms/step - loss: 0.0440 - 14x14_loss: 0.0010 - 7x7_loss: 0.0338 - val_loss: 0.0478 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0375 - lr: 2.0000e-06\n","Epoch 48/70\n","515/515 [==============================] - 152s 295ms/step - loss: 0.0436 - 14x14_loss: 9.7609e-04 - 7x7_loss: 0.0335 - val_loss: 0.0482 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0380 - lr: 2.0000e-06\n","Epoch 49/70\n","515/515 [==============================] - 152s 295ms/step - loss: 0.0437 - 14x14_loss: 9.5018e-04 - 7x7_loss: 0.0337 - val_loss: 0.0480 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0378 - lr: 2.0000e-06\n","Epoch 50/70\n","515/515 [==============================] - 152s 295ms/step - loss: 0.0429 - 14x14_loss: 9.8355e-04 - 7x7_loss: 0.0330 - val_loss: 0.0480 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0379 - lr: 2.0000e-06\n","Epoch 51/70\n","515/515 [==============================] - 152s 295ms/step - loss: 0.0427 - 14x14_loss: 9.5968e-04 - 7x7_loss: 0.0328 - val_loss: 0.0485 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0385 - lr: 2.0000e-06\n","Epoch 52/70\n","515/515 [==============================] - 152s 295ms/step - loss: 0.0431 - 14x14_loss: 9.3837e-04 - 7x7_loss: 0.0333 - val_loss: 0.0481 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0382 - lr: 4.0000e-07\n","Epoch 53/70\n","515/515 [==============================] - 152s 295ms/step - loss: 0.0422 - 14x14_loss: 9.4118e-04 - 7x7_loss: 0.0324 - val_loss: 0.0479 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0380 - lr: 4.0000e-07\n","Epoch 54/70\n","515/515 [==============================] - 152s 295ms/step - loss: 0.0425 - 14x14_loss: 9.5397e-04 - 7x7_loss: 0.0327 - val_loss: 0.0483 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0383 - lr: 4.0000e-07\n","Epoch 55/70\n","515/515 [==============================] - 152s 295ms/step - loss: 0.0423 - 14x14_loss: 9.5628e-04 - 7x7_loss: 0.0325 - val_loss: 0.0479 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0380 - lr: 4.0000e-07\n","Epoch 56/70\n","515/515 [==============================] - 152s 295ms/step - loss: 0.0424 - 14x14_loss: 9.3153e-04 - 7x7_loss: 0.0326 - val_loss: 0.0481 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0382 - lr: 4.0000e-07\n","Epoch 57/70\n","515/515 [==============================] - 152s 296ms/step - loss: 0.0425 - 14x14_loss: 9.4611e-04 - 7x7_loss: 0.0327 - val_loss: 0.0477 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0378 - lr: 8.0000e-08\n","Epoch 58/70\n","515/515 [==============================] - 152s 296ms/step - loss: 0.0422 - 14x14_loss: 9.7812e-04 - 7x7_loss: 0.0324 - val_loss: 0.0479 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0380 - lr: 8.0000e-08\n","Epoch 59/70\n","515/515 [==============================] - 153s 297ms/step - loss: 0.0425 - 14x14_loss: 9.1264e-04 - 7x7_loss: 0.0327 - val_loss: 0.0480 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0381 - lr: 8.0000e-08\n","Epoch 60/70\n","515/515 [==============================] - 152s 296ms/step - loss: 0.0420 - 14x14_loss: 9.4687e-04 - 7x7_loss: 0.0322 - val_loss: 0.0478 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0379 - lr: 8.0000e-08\n","Epoch 61/70\n","515/515 [==============================] - 152s 296ms/step - loss: 0.0428 - 14x14_loss: 9.6157e-04 - 7x7_loss: 0.0330 - val_loss: 0.0480 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0381 - lr: 8.0000e-08\n","Epoch 62/70\n","515/515 [==============================] - 160s 311ms/step - loss: 0.0421 - 14x14_loss: 9.8339e-04 - 7x7_loss: 0.0323 - val_loss: 0.0475 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0376 - lr: 1.6000e-08\n","Epoch 63/70\n","515/515 [==============================] - 153s 298ms/step - loss: 0.0422 - 14x14_loss: 9.9285e-04 - 7x7_loss: 0.0324 - val_loss: 0.0476 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0377 - lr: 1.6000e-08\n","Epoch 64/70\n","515/515 [==============================] - 152s 296ms/step - loss: 0.0423 - 14x14_loss: 9.5474e-04 - 7x7_loss: 0.0325 - val_loss: 0.0480 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0381 - lr: 1.6000e-08\n","Epoch 65/70\n","515/515 [==============================] - 153s 297ms/step - loss: 0.0423 - 14x14_loss: 9.3299e-04 - 7x7_loss: 0.0326 - val_loss: 0.0478 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0379 - lr: 1.6000e-08\n","Epoch 66/70\n","515/515 [==============================] - 153s 297ms/step - loss: 0.0422 - 14x14_loss: 9.8263e-04 - 7x7_loss: 0.0324 - val_loss: 0.0480 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0382 - lr: 1.6000e-08\n","Epoch 67/70\n","515/515 [==============================] - 153s 296ms/step - loss: 0.0423 - 14x14_loss: 9.8056e-04 - 7x7_loss: 0.0325 - val_loss: 0.0478 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0379 - lr: 1.0000e-08\n","Epoch 68/70\n","515/515 [==============================] - 152s 296ms/step - loss: 0.0424 - 14x14_loss: 9.8767e-04 - 7x7_loss: 0.0326 - val_loss: 0.0481 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0382 - lr: 1.0000e-08\n","Epoch 69/70\n","515/515 [==============================] - 152s 296ms/step - loss: 0.0420 - 14x14_loss: 9.2606e-04 - 7x7_loss: 0.0322 - val_loss: 0.0481 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0382 - lr: 1.0000e-08\n","Epoch 70/70\n","515/515 [==============================] - 153s 296ms/step - loss: 0.0423 - 14x14_loss: 9.1144e-04 - 7x7_loss: 0.0326 - val_loss: 0.0480 - val_14x14_loss: 0.0011 - val_7x7_loss: 0.0381 - lr: 1.0000e-08\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VtzLZ4MruJVM","colab_type":"text"},"source":["## Non-max suppression"]},{"cell_type":"code","metadata":{"id":"Rz2w3a_il9ba","colab_type":"code","colab":{}},"source":["# To fix multiple, we introduce non-maximum suppression, or NMS for short\n","def nms(detections, iou_thresh=0.):\n","  dets_by_class = {}\n","  final_result = []\n","  for det in detections:\n","    cls = det[1]\n","    if cls not in dets_by_class:\n","      dets_by_class[cls] = []\n","    dets_by_class[cls].append( det )\n","  for _, dets in dets_by_class.items():\n","    candidates = list(dets)\n","    candidates.sort( key=lambda x:x[0], reverse=True )\n","    while len(candidates) > 0:\n","      candidate = candidates.pop(0)\n","      _,_,cx,cy,cw,ch = candidate\n","      copy = list(candidates)\n","      for other in candidates:\n","        # Compute the IoU. If it exceeds thresh, we remove it\n","        _,_,ox,oy,ow,oh = other\n","        if iou( (cx,cy,cw,ch), (ox,oy,ow,oh) ) > iou_thresh:\n","          copy.remove(other)\n","      candidates = list(copy)\n","      final_result.append(candidate)\n","  return final_result"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DG7zDN-PuTo6","colab_type":"text"},"source":["## Load a pre-trained model"]},{"cell_type":"code","metadata":{"id":"NBvbAWSVuRDs","colab_type":"code","outputId":"cb71795d-4e66-4fd9-d062-94a418d05de4","executionInfo":{"status":"ok","timestamp":1592283734625,"user_tz":-480,"elapsed":10217,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"17512883430045220190"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["# load the model\n","load_model_path = os.path.join( load_model_folder, 'model-7x7-14x14-3aspect-modyoloposneg-wd0.0005-best_val_loss.h5' )\n","model = tf.keras.models.load_model(load_model_path, custom_objects={'custom_loss':custom_loss})"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"85U566AJuZZJ","colab_type":"code","colab":{}},"source":["# load the test data\n","label_encoder = lambda y: encode_label(y, dims_list, aspect_ratios, iou, modified_yolo_posneg_sampling, cat_list)\n","preproc_fn = lambda x: x / 255.\n","\n","test_sequence_pickle = TILPickle(val_pickle, 1, aug_identity, input_shape, label_encoder, preproc_fn)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y9ZYrvJyWNwO","colab_type":"code","colab":{}},"source":["test_sequence = TILSequence(val_imgs_folder, val_annotations, 1, aug_identity, input_shape, label_encoder, preproc_fn, testmode=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V_42ECKouX4y","colab_type":"text"},"source":["## Visualize Model Output"]},{"cell_type":"code","metadata":{"id":"v93eWhNZud3P","colab_type":"code","outputId":"854d9867-85c8-4702-f305-ff26d615bfe5","executionInfo":{"status":"ok","timestamp":1592283872579,"user_tz":-480,"elapsed":10492,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"17512883430045220190"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1e5WB9uAOQgkmwkit40yInRomDsCgCdbN"}},"source":["# Run this to visualize\n","rank_colors = ['cyan', 'magenta', 'pink']\n","det_threshold=0.\n","top_dets=3\n","\n","start=0\n","end=20\n","for k in range(start,end):\n","  img_arr, label_cxywh = test_sequence_pickle[k]\n","  img_arr = img_arr[0]\n","  pil_img = PIL.Image.fromarray( (img_arr * 255.).astype(np.uint8) )\n","  W,H = pil_img.size\n","  pred_dict = model(np.array([img_arr]))\n","  preds = decode_tensor( pred_dict, aspect_ratios )\n","    \n","  # Post-processing\n","  preds.sort( key=lambda x:x[0], reverse=True )\n","  preds = [pred for pred in preds if pred[0] >= det_threshold]\n","  preds = preds[:top_dets]\n","  preds = nms(preds, iou_thresh=0.5)\n","\n","  draw_img = pil_img.copy()\n","  draw = ImageDraw.Draw(draw_img)\n","  for i, pred in enumerate(preds):\n","    conf,cls,x,y,w,h = pred\n","    bb_x = int(x * W)\n","    bb_y = int(y * H)\n","    bb_w = int(w * W)\n","    bb_h = int(h * H)\n","    left = int(bb_x - bb_w / 2)\n","    top = int(bb_y - bb_h / 2)\n","    right = int(bb_x + bb_w / 2)\n","    bot = int(bb_y + bb_h / 2)\n","    cls_str = cat_list[cls-1]\n","\n","    draw.rectangle(((left, top), (right, bot)), outline=rank_colors[i])\n","    draw.text((bb_x, bb_y), cls_str, fill=rank_colors[i])\n","    draw.text( ( int(left + bb_w*.1), int(top + bb_h*.1) ), '{:.2f}'.format(conf), fill=rank_colors[i] )\n","\n","  display(draw_img)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"3IWZgZULSJWV","colab_type":"text"},"source":["## Generating detections"]},{"cell_type":"code","metadata":{"id":"6Xu57IkMR1jK","colab_type":"code","outputId":"7df1b201-8df8-4895-c096-4a3a601b3fc3","executionInfo":{"status":"ok","timestamp":1592282522019,"user_tz":-480,"elapsed":1109944,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"17512883430045220190"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["# Generating detections on the folder of validation images\n","detections = []\n","det_threshold=0.\n","for i in tqdm(range(len(test_sequence))):\n","  img_id, dims, input_arr, _ = test_sequence[i]\n","  img_id = int(img_id[0])\n","  W,H = dims[0]\n","\n","  # Here, I'm inferencing one-by-one, but you can batch it if you want it faster\n","  pred_dict = model(input_arr)\n","  preds = decode_tensor( pred_dict, aspect_ratios )\n","\n","  # Post-processing\n","  preds = [pred for pred in preds if pred[0] >= det_threshold]\n","  preds.sort( key=lambda x:x[0], reverse=True )\n","  preds = preds[:100] # we only evaluate you on 100 detections per image\n","\n","  for i, pred in enumerate(preds):\n","    conf,cat_id,x,y,w,h = pred\n","    left = W * (x - w/2.)\n","    left = round(left,1)\n","    top = H * (y - h/2.)\n","    top = round(top,1)\n","    width = W*w\n","    width = round(width,1)\n","    height = H*h\n","    height = round(height,1)\n","    conf = float(conf)\n","    cat_id = int(cat_id)\n","    detections.append( {'image_id':img_id, 'category_id':cat_id, 'bbox':[left, top, width, height], 'score':conf} )\n","\n","with open('detections-7x7-14x14-top100-nonms.json', 'w') as f:\n","  json.dump(detections, f)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|| 1474/1474 [18:26<00:00,  1.33it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"iMOqbqfkSJKO","colab_type":"code","outputId":"4f6b1757-ccef-488a-97dc-a9d6de0cdedb","executionInfo":{"status":"ok","timestamp":1592283892128,"user_tz":-480,"elapsed":11156,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"17512883430045220190"}},"colab":{"base_uri":"https://localhost:8080/","height":372}},"source":["#This installation is a modified version of the original to suit this competition\n","! pip install git+https://github.com/jinmingteo/cocoapi.git#subdirectory=PythonAPI"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/jinmingteo/cocoapi.git#subdirectory=PythonAPI\n","  Cloning https://github.com/jinmingteo/cocoapi.git to /tmp/pip-req-build-1k1cp3jn\n","  Running command git clone -q https://github.com/jinmingteo/cocoapi.git /tmp/pip-req-build-1k1cp3jn\n","Requirement already satisfied (use --upgrade to upgrade): pycocotools==2.0 from git+https://github.com/jinmingteo/cocoapi.git#subdirectory=PythonAPI in /usr/local/lib/python3.6/dist-packages\n","Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (47.1.1)\n","Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (0.29.19)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (3.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.17.4)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.2.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools==2.0) (1.12.0)\n","Building wheels for collected packages: pycocotools\n","  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=267092 sha256=ee2859bed711b5f0817ff338b437ec43a6d10b90de5dafff3c31a0576858b884\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-88tyuzj5/wheels/27/81/92/3a512329d1b1ae7fc278285a1f114ef08082568bf32eee0002\n","Successfully built pycocotools\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FnEFeuvUdiKK","colab_type":"code","colab":{}},"source":["from pycocotools.coco import COCO\n","from pycocotools.cocoeval import COCOeval"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"amv4RY1Ndo2H","colab_type":"code","outputId":"8997b833-cabc-46cb-958f-d32ac4d2bb0a","executionInfo":{"status":"ok","timestamp":1592283923702,"user_tz":-480,"elapsed":8505,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"17512883430045220190"}},"colab":{"base_uri":"https://localhost:8080/","height":482}},"source":["# Get evaluation score against validation set\n","coco_gt = COCO(val_annotations)\n","coco_dt = coco_gt.loadRes('./gdrive/My Drive/datasets/detections-7x7-14x14-top100-nonms.json')\n","cocoEval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')\n","cocoEval.evaluate()\n","cocoEval.accumulate()\n","cocoEval.summarize()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["loading annotations into memory...\n","Done (t=0.02s)\n","creating index...\n","index created!\n","Loading and preparing results...\n","DONE (t=0.95s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","DONE (t=4.95s).\n","Accumulating evaluation results...\n","DONE (t=1.50s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.004\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.019\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.004\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.017\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.090\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.108\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.110\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IczmnrU8hF3s","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UJ2xmDV6drOQ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}