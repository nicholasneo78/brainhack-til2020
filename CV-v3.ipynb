{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 814,
     "status": "ok",
     "timestamp": 1592545320232,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "UT3GnHJaEziz",
    "outputId": "4b9792b9-eaa8-48d8-df29-e60a0303a20a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 821,
     "status": "ok",
     "timestamp": 1592545321469,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "PFppAGwlE9QO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4290,
     "status": "ok",
     "timestamp": 1592545325299,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "Hj_lRwSVfll2",
    "outputId": "1dedac4e-9ca7-42ab-f98a-d0e44b9de3f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.17.4 in /usr/local/lib/python3.6/dist-packages (1.17.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.17.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 873,
     "status": "ok",
     "timestamp": 1592545327158,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "ZnhFcyR6wjwV",
    "outputId": "7755a87b-6b56-463b-87f8-3d2801935748"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.17.4'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5o0x2LsYHnVr"
   },
   "source": [
    "## Test the file directory of google drive with dummy csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 868,
     "status": "ok",
     "timestamp": 1592545328355,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "EvopVDVaGga3",
    "outputId": "09af7bfc-bd17-435a-d651-52a0a0ef9d07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1728,
     "status": "ok",
     "timestamp": 1592545329424,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "rfMFntMpGafu"
   },
   "outputs": [],
   "source": [
    "a = pd.read_csv(\"./gdrive/My Drive/datasets/dummy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1552,
     "status": "ok",
     "timestamp": 1592545329425,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "BM2aUBUxGpfL",
    "outputId": "a0468185-bbda-4e65-cc21-41776fa6c02e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shopid</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6042309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104804492</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8715449</td>\n",
       "      <td>9753706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>190969466</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2859407</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      shopid   userid\n",
       "0    6042309        0\n",
       "1  104804492        0\n",
       "2    8715449  9753706\n",
       "3  190969466        0\n",
       "4    2859407        0"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k9y4D5IfHuVI"
   },
   "source": [
    "## Actual CV Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2666,
     "status": "ok",
     "timestamp": 1592545330890,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "RouIPsTCHwma"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from math import log, exp\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from PIL import ImageEnhance, ImageFont, ImageDraw\n",
    "from IPython.display import Image, display\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2466,
     "status": "ok",
     "timestamp": 1592545330892,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "7-rrTqzDITkg"
   },
   "outputs": [],
   "source": [
    "cat_list = ['tops', 'trousers', 'outerwear', 'dresses', 'skirts']\n",
    "\n",
    "input_shape = (224,224,3)\n",
    "wt_decay = 5e-4\n",
    "dims_list = [(7,7),(14,14)]\n",
    "aspect_ratios = [(1,1), (1,2), (2,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2338,
     "status": "ok",
     "timestamp": 1592545330892,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "Q01wn6aNIV5b"
   },
   "outputs": [],
   "source": [
    "# Set up directory\n",
    "data_folder = './gdrive/My Drive/datasets/'\n",
    "submission_folder = os.path.join(data_folder, 'submissions/')\n",
    "train_imgs_folder = os.path.join(data_folder,'train/', 'train/')\n",
    "train_annotations = os.path.join(data_folder,'train.json')\n",
    "val_imgs_folder = os.path.join(data_folder,'val/','val/')\n",
    "val_annotations = os.path.join(data_folder,'val.json')\n",
    "train_pickle = os.path.join( data_folder, 'train.p/','train.p')\n",
    "val_pickle = os.path.join( data_folder, 'val.p/','val.p')\n",
    "\n",
    "test_imgs_folder = os.path.join(data_folder,'CV_interim_images/')\n",
    "test_annotations = os.path.join(data_folder,'CV_interim_evaluation.json')\n",
    "\n",
    "save_model_folder = submission_folder\n",
    "load_model_folder = submission_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2185,
     "status": "ok",
     "timestamp": 1592545330893,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "nBuDUwC5IdYn",
    "outputId": "988abcbf-b85a-4393-cc82-d7d4e78845bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./gdrive/My Drive/datasets/train/train/\n",
      "./gdrive/My Drive/datasets/train.json\n",
      "./gdrive/My Drive/datasets/val/val/\n",
      "./gdrive/My Drive/datasets/val.json\n",
      "./gdrive/My Drive/datasets/CV_interim_images/\n",
      "./gdrive/My Drive/datasets/CV_interim_evaluation.json\n",
      "./gdrive/My Drive/datasets/train.p/train.p\n",
      "./gdrive/My Drive/datasets/val.p/val.p\n",
      "./gdrive/My Drive/datasets/submissions/\n",
      "./gdrive/My Drive/datasets/submissions/\n",
      "./gdrive/My Drive/datasets/submissions/\n"
     ]
    }
   ],
   "source": [
    "# Check if the directories are correct\n",
    "print(train_imgs_folder)\n",
    "print(train_annotations)\n",
    "print(val_imgs_folder)\n",
    "print(val_annotations)\n",
    "print(test_imgs_folder)\n",
    "print(test_annotations)\n",
    "print(train_pickle)\n",
    "print(val_pickle)\n",
    "print(submission_folder)\n",
    "print(save_model_folder)\n",
    "print(load_model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2006,
     "status": "ok",
     "timestamp": 1592545330894,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "U87MX6fsIgC-",
    "outputId": "21799c19-e625-4819-b97d-680880477fd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwhKCBvlIyu_"
   },
   "source": [
    "## Data Augmentation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1682,
     "status": "ok",
     "timestamp": 1592545330895,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "gEB7brBFIjBK"
   },
   "outputs": [],
   "source": [
    "# Helper methods: Computes the boundary of the image that includes all bboxes\n",
    "\n",
    "def compute_reasonable_boundary(labels):\n",
    "    bounds = [ (x-w/2, x+w/2, y-h/2, y+h/2) for _,x,y,w,h in labels]\n",
    "    xmin = min([bb[0] for bb in bounds])\n",
    "    xmax = max([bb[1] for bb in bounds])\n",
    "    ymin = min([bb[2] for bb in bounds])\n",
    "    ymax = max([bb[3] for bb in bounds])\n",
    "    return xmin, xmax, ymin, ymax\n",
    "\n",
    "def aug_horizontal_flip(img, labels):\n",
    "    flipped_labels = []\n",
    "    for c,x,y,w,h in labels:\n",
    "        flipped_labels.append( (c,1-x,y,w,h) )\n",
    "    return img.transpose(PIL.Image.FLIP_LEFT_RIGHT), np.array(flipped_labels)\n",
    "\n",
    "def aug_crop(img, labels):\n",
    "    # Compute bounds such that no boxes are cut out\n",
    "    xmin, xmax, ymin, ymax = compute_reasonable_boundary(labels)\n",
    "    # Choose crop_xmin from [0, xmin]\n",
    "    crop_xmin = max( np.random.uniform() * xmin, 0 )\n",
    "    # Choose crop_xmax from [xmax, 1]\n",
    "    crop_xmax = min( xmax + (np.random.uniform() * (1-xmax)), 1 )\n",
    "    # Choose crop_ymin from [0, ymin]\n",
    "    crop_ymin = max( np.random.uniform() * ymin, 0 )\n",
    "    # Choose crop_ymax from [ymax, 1]\n",
    "    crop_ymax = min( ymax + (np.random.uniform() * (1-ymax)), 1 )\n",
    "    # Compute the \"new\" width and height of the cropped image\n",
    "    crop_w = crop_xmax - crop_xmin\n",
    "    crop_h = crop_ymax - crop_ymin\n",
    "    cropped_labels = []\n",
    "    for c,x,y,w,h in labels:\n",
    "        c_x = (x - crop_xmin) / crop_w\n",
    "        c_y = (y - crop_ymin) / crop_h\n",
    "        c_w = w / crop_w\n",
    "        c_h = h / crop_h\n",
    "        cropped_labels.append( (c,c_x,c_y,c_w,c_h) )\n",
    "\n",
    "    W,H = img.size\n",
    "    # Compute the pixel coordinates and perform the crop\n",
    "    impix_xmin = int(W * crop_xmin)\n",
    "    impix_xmax = int(W * crop_xmax)\n",
    "    impix_ymin = int(H * crop_ymin)\n",
    "    impix_ymax = int(H * crop_ymax)\n",
    "    return img.crop( (impix_xmin, impix_ymin, impix_xmax, impix_ymax) ), np.array( cropped_labels )\n",
    "\n",
    "def aug_translate(img, labels):\n",
    "    # Compute bounds such that no boxes are cut out\n",
    "    xmin, xmax, ymin, ymax = compute_reasonable_boundary(labels)\n",
    "    trans_range_x = [-xmin, 1 - xmax]\n",
    "    tx = trans_range_x[0] + (np.random.uniform() * (trans_range_x[1] - trans_range_x[0]))\n",
    "    trans_range_y = [-ymin, 1 - ymax]\n",
    "    ty = trans_range_y[0] + (np.random.uniform() * (trans_range_y[1] - trans_range_y[0]))\n",
    "\n",
    "    trans_labels = []\n",
    "    for c,x,y,w,h in labels:\n",
    "        trans_labels.append((c,x+tx,y+ty,w,h))\n",
    "\n",
    "    W,H = img.size\n",
    "    tx_pix = int(W * tx)\n",
    "    ty_pix = int(H * ty)\n",
    "    return img.rotate(0, translate=(tx_pix, ty_pix)), np.array( trans_labels )\n",
    "\n",
    "def aug_colorbalance(img, labels, color_factors=[0.2,2.0]):\n",
    "    factor = color_factors[0] + np.random.uniform() * (color_factors[1] - color_factors[0])\n",
    "    enhancer = ImageEnhance.Color(img)\n",
    "    return enhancer.enhance(factor), labels\n",
    "\n",
    "def aug_contrast(img, labels, contrast_factors=[0.2,2.0]):\n",
    "    factor = contrast_factors[0] + np.random.uniform() * (contrast_factors[1] - contrast_factors[0])\n",
    "    enhancer = ImageEnhance.Contrast(img)\n",
    "    return enhancer.enhance(factor), labels\n",
    "\n",
    "def aug_brightness(img, labels, brightness_factors=[0.2,2.0]):\n",
    "    factor = brightness_factors[0] + np.random.uniform() * (brightness_factors[1] - brightness_factors[0])\n",
    "    enhancer = ImageEnhance.Brightness(img)\n",
    "    return enhancer.enhance(factor), labels\n",
    "\n",
    "def aug_sharpness(img, labels, sharpness_factors=[0.2,10.0]):\n",
    "    factor = sharpness_factors[0] + np.random.uniform() * (sharpness_factors[1] - sharpness_factors[0])\n",
    "    enhancer = ImageEnhance.Sharpness(img)\n",
    "    return enhancer.enhance(factor), labels\n",
    "\n",
    "# Performs no augmentations and returns the original image and bbox. Used for the validation images.\n",
    "def aug_identity(pil_img, label_arr):\n",
    "    return np.array(pil_img), label_arr\n",
    "\n",
    "# This is the default augmentation scheme that we will use for each training image.\n",
    "def aug_default(img, labels, p={'flip':0.5, 'crop':0.5, 'translate':0.5, 'color':0.2, 'contrast':0.2, 'brightness':0.2, 'sharpness':0.2}):\n",
    "    if p['color'] > np.random.uniform():\n",
    "        img, labels = aug_colorbalance(img, labels)\n",
    "    if p['contrast'] > np.random.uniform():\n",
    "        img, labels = aug_contrast(img, labels)\n",
    "    if p['brightness'] > np.random.uniform():\n",
    "        img, labels = aug_brightness(img, labels)\n",
    "    if p['sharpness'] > np.random.uniform():\n",
    "        img, labels = aug_sharpness(img, labels)\n",
    "    if p['flip'] > np.random.uniform():\n",
    "        img, labels = aug_horizontal_flip(img, labels)\n",
    "    if p['crop'] > np.random.uniform():\n",
    "        img, labels = aug_crop(img, labels)\n",
    "    if p['translate'] > np.random.uniform():\n",
    "        img, labels = aug_translate(img, labels)\n",
    "    return np.array(img), labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DD-rtvHTI0pS"
   },
   "source": [
    "## Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1361,
     "status": "ok",
     "timestamp": 1592545330896,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "1srd_S6OIlNk"
   },
   "outputs": [],
   "source": [
    "# Shape of ypred: ( batch, i, j, aspect_ratios, 7 ). For a batch,i,j, we get #aspect_ratios vectors of length 7.\n",
    "# Shape of ytrue: ( batch, i, j, aspect_ratios, 9 ). For a batch,i,j, we get #aspect_ratios vectors of length 9 (two more for objectness and cat/loc indicators)\n",
    "def custom_loss(ytrue, ypred):\n",
    "    obj_loss_weight = 1.0\n",
    "    cat_loss_weight = 1.0\n",
    "    loc_loss_weight = 1.0\n",
    "\n",
    "    end_cat = len(cat_list) + 1\n",
    "\n",
    "    objloss_indicators = ytrue[:,:,:,:,-2:-1]\n",
    "    catlocloss_indicators = ytrue[:,:,:,:,-1:]\n",
    "\n",
    "    ytrue_obj, ypred_obj = ytrue[:,:,:,:,:1], ypred[:,:,:,:,:1]\n",
    "    ytrue_obj = tf.where( objloss_indicators != 0, ytrue_obj, 0 )\n",
    "    ypred_obj = tf.where( objloss_indicators != 0, ypred_obj, 0 )\n",
    "    objectness_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)( ytrue_obj, ypred_obj )\n",
    "\n",
    "    ytrue_cat, ypred_cat = ytrue[:,:,:,:,1:end_cat], ypred[:,:,:,:,1:end_cat]\n",
    "    ytrue_cat = tf.where( catlocloss_indicators != 0, ytrue_cat, 0 )\n",
    "    ypred_cat = tf.where( catlocloss_indicators != 0, ypred_cat, 0 )\n",
    "    categorical_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True) ( ytrue_cat, ypred_cat )\n",
    "\n",
    "    # Remember that ytrue is longer than ypred, so we will need to stop at index -2, which is where the indicators are stored\n",
    "    ytrue_loc, ypred_loc = ytrue[:,:,:,:,end_cat:-2], ypred[:,:,:,:,end_cat:]\n",
    "    ytrue_loc = tf.where( catlocloss_indicators != 0, ytrue_loc, 0 )\n",
    "    ypred_loc = tf.where( catlocloss_indicators != 0, ypred_loc, 0 )\n",
    "    localisation_loss = tf.keras.losses.Huber() ( ytrue_loc, ypred_loc )\n",
    "\n",
    "    return obj_loss_weight*objectness_loss + cat_loss_weight*categorical_loss + loc_loss_weight*localisation_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P22FJ9JNI6Lv"
   },
   "source": [
    "## IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1018,
     "status": "ok",
     "timestamp": 1592545330897,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "yJNl2i-SIn7D"
   },
   "outputs": [],
   "source": [
    "# Computes the intersection-over-union (IoU) of two bounding boxes\n",
    "def iou(bb1, bb2):\n",
    "    x1,y1,w1,h1 = bb1\n",
    "    xmin1 = x1 - w1/2\n",
    "    xmax1 = x1 + w1/2\n",
    "    ymin1 = y1 - h1/2\n",
    "    ymax1 = y1 + h1/2\n",
    "\n",
    "    x2,y2,w2,h2 = bb2\n",
    "    xmin2 = x2 - w2/2\n",
    "    xmax2 = x2 + w2/2\n",
    "    ymin2 = y2 - h2/2\n",
    "    ymax2 = y2 + h2/2\n",
    "\n",
    "    area1 = w1*h1\n",
    "    area2 = w2*h2\n",
    "\n",
    "    # Compute the boundary of the intersection\n",
    "    xmin_int = max(xmin1, xmin2)\n",
    "    xmax_int = min(xmax1, xmax2)\n",
    "    ymin_int = max(ymin1, ymin2)\n",
    "    ymax_int = min(ymax1, ymax2)\n",
    "    intersection = max(xmax_int - xmin_int, 0) * max(ymax_int - ymin_int, 0)\n",
    "\n",
    "    # Remove the double counted region\n",
    "    union = area1+area2-intersection\n",
    "\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCf8Z3IzJAeZ"
   },
   "source": [
    "## Sampling Schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1107,
     "status": "ok",
     "timestamp": 1592545331341,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "98QIHd94IrKL"
   },
   "outputs": [],
   "source": [
    "# Sampling schemes\n",
    "def yolo_posneg_sampling(iou_scores_dict, label_tensor, gtclass, cat_list, iou_threshold=0.5):\n",
    "    iou_scores = []\n",
    "    for _, scores in iou_scores_dict.items():\n",
    "        iou_scores.extend(scores)\n",
    "    iou_scores.sort( key=lambda x: x[0], reverse=True )\n",
    "  \n",
    "    top_iou_score = iou_scores.pop(0)\n",
    "    _, key, i, j, k, dx, dy, dw, dh = top_iou_score\n",
    "    zeros = [0] * len(cat_list)\n",
    "    payload = [1, *zeros, dx,dy,dw,dh]\n",
    "    payload[gtclass + 1] = 1\n",
    "    # Train objectness, class and loc for the positive\n",
    "    label_tensor[key][i,j,k,-2:] = 1\n",
    "    label_tensor[key][i,j,k,:len(payload)] = payload\n",
    "\n",
    "    # Train objectness only for the negatives\n",
    "    low_iou_scores = [iou_score for iou_score in iou_scores if iou_score[0] < iou_threshold]\n",
    "    for _, key, i, j, k, _, _, _, _ in low_iou_scores:\n",
    "        label_tensor[key][i,j,k,-2] = 1\n",
    "\n",
    "def modified_yolo_posneg_sampling(iou_scores_dict, label_tensor, gtclass, cat_list, iou_threshold=0.5):\n",
    "    iou_scores = []\n",
    "    zeros = [0] * len(cat_list)\n",
    "\n",
    "    for _, scores in iou_scores_dict.items():\n",
    "        iou_scores.extend(scores)\n",
    "    iou_scores.sort( key=lambda x: x[0], reverse=True )\n",
    "  \n",
    "    top_iou_score = iou_scores.pop(0)\n",
    "    _, key, i, j, k, dx, dy, dw, dh = top_iou_score\n",
    "    payload = [1, *zeros, dx,dy,dw,dh]\n",
    "    payload[gtclass + 1] = 1\n",
    "    # Train objectness, class and loc for the positive\n",
    "    label_tensor[key][i,j,k,-2:] = 1\n",
    "    label_tensor[key][i,j,k,:len(payload)] = payload\n",
    "\n",
    "    # Train objectness only for the negatives\n",
    "    low_iou_scores = [iou_score for iou_score in iou_scores if iou_score[0] < iou_threshold]\n",
    "    for _, key, i, j, k, _, _, _, _ in low_iou_scores:\n",
    "        label_tensor[key][i,j,k,-2] = 1\n",
    "\n",
    "    # Train cat/loc only for the in-betweens - those with high IoU but not positive\n",
    "    high_iou_scores = [iou_score for iou_score in iou_scores if iou_score[0] >= iou_threshold]\n",
    "    for _, key, i, j, k, dx, dy, dw, dh in high_iou_scores:\n",
    "        label_tensor[key][i,j,k,-1] = 1\n",
    "        payload = [0,*zeros,dx,dy,dw,dh]\n",
    "        payload[gtclass + 1] = 1\n",
    "        label_tensor[key][i,j,k,:len(payload)] = payload\n",
    "\n",
    "def top_ratio_sampling(iou_scores_dict, label_tensor, gtclass, cat_list, positive_ratio=0.25):\n",
    "    iou_scores = []\n",
    "    # Let all tensors learn objectness score\n",
    "    for v in label_tensor.values():\n",
    "        v[:,:,:,-2] = 1\n",
    "  \n",
    "    for _, iou_score_list in iou_scores_dict.items():\n",
    "        iou_score_list.sort( key=lambda x: x[0], reverse=True )\n",
    "        top_percentile_iou_scores = iou_score_list[:round(len(iou_score_list) * positive_ratio)]\n",
    "        # Include the rest that cross the IoU threshold\n",
    "        iou_score_list = top_percentile_iou_scores + [iou_score for iou_score in iou_score_list[len(top_percentile_iou_scores):] if iou_score[0] >= self.iou_threshold]\n",
    "        iou_scores.extend( iou_score_list )\n",
    "\n",
    "    for iou_score in iou_scores:\n",
    "        IoU, key, i, j, k, dx, dy, dw, dh = iou_score\n",
    "        zeros = [0] * len(cat_list)\n",
    "        payload = [IoU, *zeros, dx,dy,dw,dh]\n",
    "        payload[gtclass + 1] = 1\n",
    "        label_tensor[key][i,j,k,:len(payload)] = payload\n",
    "        # Set the classification/localisation indicator at this location to positive\n",
    "        label_tensor[key][i,j,k,-1] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j48ueWUTJJI2"
   },
   "source": [
    "## Encoding labels/ Decoding model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1233,
     "status": "ok",
     "timestamp": 1592545331813,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "eBO__QLLJKWA"
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Encoder: label -> tensor\n",
    "label_arr: np array like:\n",
    "[[class_idx x y w h]]: num_labels x 5\n",
    "...  \n",
    "Used to figure out for each label line, which tensor entry to shove it into.\n",
    "If the box corresponding to the tensor entry overlaps the ground truth by at least a predefined threshold, then we shove it in.\n",
    "'''\n",
    "\n",
    "def encode_label(label_arr, dims_list, aspect_ratios, iou_fn, sampling_fn, cat_list):\n",
    "    num_entries = 7 + len(cat_list) # objectness, ... len(cat_list) ..., dx, dy, dw, dh, obj_indicator, catloc_indicator\n",
    "    np_labels = {}\n",
    "    for dims in dims_list:\n",
    "        dimkey = '{}x{}'.format(*dims)\n",
    "        np_labels[dimkey] = np.zeros( (*dims, len(aspect_ratios), num_entries ) )\n",
    "\n",
    "    for label in label_arr:\n",
    "        gtclass, gtx, gty, gtw, gth = label\n",
    "        gtclass = int(gtclass)\n",
    "        gt_bbox = [gtx, gty, gtw, gth]\n",
    "    \n",
    "        iou_scores_dict = {}\n",
    "\n",
    "        for dims in dims_list:\n",
    "            key = '{}x{}'.format(*dims)\n",
    "    \n",
    "            kx,ky = dims\n",
    "            gapx = 1.0 / kx\n",
    "            gapy = 1.0 / ky\n",
    "            \n",
    "\n",
    "            # There are kx x ky tiles. \n",
    "            # For now, all have the same w,h of gapx,gapy. \n",
    "            # For the (i,j)-th tile, x = 0.5*gapx + i*gapx = (0.5+i)*gapx | y = (0.5+j)*gapy\n",
    "            \n",
    "            for i in range(kx):\n",
    "                for j in range(ky):\n",
    "                    for k in range( len(aspect_ratios) ):\n",
    "                        dims_aspect_key = (*dims, k) # a 3-tuple: (dim1,dim2,ar)\n",
    "                        if dims_aspect_key not in iou_scores_dict:\n",
    "                            iou_scores_dict[dims_aspect_key] = []\n",
    "                        x = (0.5+i)*gapx\n",
    "                        y = (0.5+j)*gapy\n",
    "\n",
    "                        # Different aspect ratios alter the anchor box default dimensions\n",
    "                        w = gapx * aspect_ratios[k][0]\n",
    "                        h = gapy * aspect_ratios[k][1]\n",
    "                        cand_bbox = [x,y,w,h]\n",
    "\n",
    "                        # SSD formulation\n",
    "                        dx = (gtx - x) / w \n",
    "                        dy = (gty - y) / h\n",
    "                        dw = log( gtw / w )\n",
    "                        dh = log( gth / h )\n",
    "            \n",
    "                        int_over_union = iou_fn( cand_bbox, gt_bbox )\n",
    "                        iou_scores_dict[dims_aspect_key].append( (int_over_union, key, i, j, k, dx, dy, dw, dh) )\n",
    "            sampling_fn( iou_scores_dict, np_labels, gtclass, cat_list )\n",
    "    return np_labels\n",
    "\n",
    "def decode_tensor(pred_dict, aspect_ratios):\n",
    "    results = []\n",
    "    for dim_str, pred_tensor in pred_dict.items():\n",
    "        pred_tensor = pred_tensor[0] # remove the batch\n",
    "        kx, ky = [int(g) for g in dim_str.split('x')]\n",
    "        gapx = 1. / kx\n",
    "        gapy = 1. / ky\n",
    "\n",
    "        # We trained without activations, so we need to process the logits into probabilities/scores\n",
    "        pred_arr = np.array(pred_tensor)\n",
    "        obj_logits = pred_arr[:,:,:,0]\n",
    "        obj_scores = 1. / (1 + np.exp(-obj_logits))\n",
    "        pred_arr[:,:,:,0] = obj_scores\n",
    "\n",
    "        cls_logits = pred_arr[:,:,:,1:-4]\n",
    "        cls_scores = np.exp(cls_logits)\n",
    "        cls_scores = cls_scores / cls_scores.sum(axis=-1)[...,np.newaxis]\n",
    "        pred_arr[:,:,:,1:-4] = cls_scores\n",
    "\n",
    "        for k, ar in enumerate(aspect_ratios):\n",
    "            for i in range(kx):\n",
    "                for j in range(ky):\n",
    "                    cx = (0.5+i)*gapx\n",
    "                    cy = (0.5+j)*gapy\n",
    "                    w = gapx * ar[0]\n",
    "                    h = gapy * ar[1]\n",
    "\n",
    "                    payload = pred_arr[i,j,k]\n",
    "                    obj_score = payload[0]\n",
    "                    dx, dy, dw, dh = payload[-4:]\n",
    "                    cls_probs = payload[1:-4]\n",
    "\n",
    "                    predx = (dx * w) + cx\n",
    "                    predy = (dy * h) + cy\n",
    "                    predw = w * exp( dw )\n",
    "                    predh = h * exp( dh )\n",
    "                    max_cls_idx = np.argmax( cls_probs )\n",
    "                    max_cls_prob = cls_probs[max_cls_idx]\n",
    "                    category_id = max_cls_idx + 1\n",
    "                    det_score = obj_score * max_cls_prob\n",
    "                    results.append( (det_score, category_id, predx, predy, predw, predh) )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IhMy4TlYJW4j"
   },
   "source": [
    "## Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1382,
     "status": "ok",
     "timestamp": 1592545332337,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "CZ_snVQlJROm"
   },
   "outputs": [],
   "source": [
    "class TILSequence(Sequence):\n",
    "\n",
    "    def __init__(self, img_folder, json_annotation_file, batch_size, augment_fn, input_size, label_encoder, preprocess_fn, testmode=False):\n",
    "        self._prepare_data(img_folder, json_annotation_file)\n",
    "        self.batch_size = batch_size\n",
    "        self.augment_fn = augment_fn\n",
    "        self.input_wh = (*input_size[:2][::-1],input_size[2])\n",
    "        self.label_encoder = label_encoder\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        self.testmode = testmode\n",
    "    \n",
    "    def _prepare_data(self, img_folder, json_annotation_file):\n",
    "        imgs_dict = {im.split('.')[0]:im for im in os.listdir(img_folder) if im.endswith('.jpg')}\n",
    "        data_dict = {}\n",
    "        with open(json_annotation_file, 'r') as f:\n",
    "            annotations_dict = json.load(f)\n",
    "        annotations_list = annotations_dict['annotations']\n",
    "        for annotation in annotations_list:\n",
    "            img_id = str(annotation['image_id'])\n",
    "            c = annotation['category_id'] - 1 # TODO: make sure that category ids start from 1, not 0\n",
    "            boxleft,boxtop,boxwidth,boxheight = annotation['bbox']\n",
    "            if img_id in imgs_dict:\n",
    "                img_fp = os.path.join(img_folder, imgs_dict[img_id])\n",
    "                imwidth,imheight = PIL.Image.open(img_fp).size\n",
    "                if img_id not in data_dict:\n",
    "                    data_dict[img_id] = []\n",
    "                box_cenx = boxleft + boxwidth/2.\n",
    "                box_ceny = boxtop + boxheight/2.\n",
    "                x,y,w,h = box_cenx/imwidth, box_ceny/imheight, boxwidth/imwidth, boxheight/imheight\n",
    "\n",
    "                data_dict[img_id].append( [c,x,y,w,h] )\n",
    "        self.x, self.y, self.ids = [], [], []\n",
    "        for img_id, labels in data_dict.items():\n",
    "            self.x.append( os.path.join(img_folder, imgs_dict[img_id]) )\n",
    "            self.y.append( np.array(labels) )\n",
    "            self.ids.append( img_id )\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_batch_test(idx) if self.testmode else self.get_batch(idx)\n",
    "\n",
    "    def get_batch_test(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_ids = self.ids[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        x_acc, y_acc = [], {}\n",
    "        original_img_dims = []\n",
    "        with Pool(self.batch_size) as p:\n",
    "            # Read in the PIL objects from filepaths\n",
    "            batch_x = p.map(load_img, batch_x)\n",
    "    \n",
    "        for x,y in zip( batch_x, batch_y ):\n",
    "            W,H = x.size\n",
    "            original_img_dims.append( (W,H) )\n",
    "            x_aug, y_aug = self.augment_fn( x, y )\n",
    "            if x_aug.size != self.input_wh[:2]:\n",
    "                x_aug.resize( self.input_wh )\n",
    "            x_acc.append( np.array(x_aug) )\n",
    "            y_dict = self.label_encoder( y_aug )\n",
    "            for dimkey, label in y_dict.items():\n",
    "                if dimkey not in y_acc:\n",
    "                    y_acc[dimkey] = []\n",
    "                y_acc[dimkey].append( label )\n",
    "\n",
    "        return batch_ids, original_img_dims, self.preprocess_fn( np.array( x_acc ) ), { dimkey: np.array( gt_tensor ) for dimkey, gt_tensor in y_acc.items() }\n",
    "\n",
    "    def get_batch(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        x_acc, y_acc = [], {}\n",
    "        with Pool(self.batch_size) as p:\n",
    "            # Read in the PIL objects from filepaths\n",
    "            batch_x = p.map(load_img, batch_x)\n",
    "\n",
    "        for x,y in zip( batch_x, batch_y ):\n",
    "            x_aug, y_aug = self.augment_fn( x, y )\n",
    "            if x_aug.size != self.input_wh[:2]:\n",
    "                x_aug.resize( self.input_wh )\n",
    "            x_acc.append( np.array(x_aug) )\n",
    "            y_dict = self.label_encoder( y_aug )\n",
    "            for dimkey, label in y_dict.items():\n",
    "                if dimkey not in y_acc:\n",
    "                    y_acc[dimkey] = []\n",
    "                y_acc[dimkey].append( label )\n",
    "\n",
    "        return self.preprocess_fn( np.array( x_acc ) ), { dimkey: np.array( gt_tensor ) for dimkey, gt_tensor in y_acc.items() }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1227,
     "status": "ok",
     "timestamp": 1592545332339,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "7_s5AA9mJZui"
   },
   "outputs": [],
   "source": [
    "class TILPickle(Sequence):\n",
    "    def __init__(self, pickle_file, batch_size, augment_fn, input_size, label_encoder, preprocess_fn, testmode=False):\n",
    "        with open(pickle_file, 'rb') as p:\n",
    "            self.ids, self.x, self.y = pickle.load(p)\n",
    "        self.batch_size = batch_size\n",
    "        self.augment_fn = augment_fn\n",
    "        self.input_wh = (*input_size[:2][::-1],input_size[2])\n",
    "        self.label_encoder = label_encoder\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        self.testmode = testmode\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_ids = self.ids[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        x_acc, y_acc = [], {}\n",
    "    \n",
    "        for x,y in zip( batch_x, batch_y ):\n",
    "            x_aug, y_aug = self.augment_fn( x, y )\n",
    "            if x_aug.size != self.input_wh[:2]:\n",
    "                x_aug.resize( self.input_wh )\n",
    "            x_acc.append( np.array(x_aug) )\n",
    "            y_dict = self.label_encoder( y_aug )\n",
    "            for dimkey, label in y_dict.items():\n",
    "                if dimkey not in y_acc:\n",
    "                    y_acc[dimkey] = []\n",
    "                y_acc[dimkey].append(label)\n",
    "\n",
    "        if self.testmode:\n",
    "            return batch_ids, self.preprocess_fn( np.array( x_acc ) ), { dimkey: np.array( gt_tensor ) for dimkey, gt_tensor in y_acc.items() }\n",
    "        return self.preprocess_fn( np.array( x_acc ) ), { dimkey: np.array( gt_tensor ) for dimkey, gt_tensor in y_acc.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6wxIQJBBJ-0u"
   },
   "source": [
    "## Constructing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 857,
     "status": "ok",
     "timestamp": 1592545332340,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "cbA9GtEgJ_c6"
   },
   "outputs": [],
   "source": [
    "def transfer_model_7x7_14x14(backbone_model, input_shape, dims_list, num_aspect_ratios, wt_decay, model_name='transfer-objdet-model-7x7-14x14'):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    intermediate_layer_model = keras.Model(inputs=backbone_model.input,\n",
    "                                         outputs=backbone_model.get_layer('conv4_block6_out').output)\n",
    "    intermediate_output = intermediate_layer_model(inputs) #14\n",
    "    backbone_output = backbone_model(inputs) #7\n",
    "\n",
    "    x = layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay))(backbone_output) #7\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(1024, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #7\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #7\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(1024, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #7\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #7\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    upsample = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(2048, 3, padding='same', kernel_regularizer=l2(wt_decay))(upsample) #7\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    tens_7x7 = layers.Add()([x,backbone_output])\n",
    "\n",
    "    x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(upsample) #7\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2DTranspose(512, 5, strides=(2, 2), padding='same')(x) #14\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "\n",
    "    x = layers.Concatenate()([x,intermediate_output])\n",
    "\n",
    "    x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    tens_14x14 = layers.LeakyReLU(0.01)(x)\n",
    "\n",
    "    dim_tensor_map = {'7x7': tens_7x7, '14x14': tens_14x14}\n",
    "\n",
    "    # For each dimension, construct a predictions tensor. Accumulate them into a dictionary for keras to understand multiple labels.\n",
    "    preds_dict = {}\n",
    "    for dims in dims_list:\n",
    "        dimkey = '{}x{}'.format(*dims)\n",
    "        tens = dim_tensor_map[dimkey]\n",
    "        ar_preds = []\n",
    "        for _ in range(num_aspect_ratios):\n",
    "            objectness_preds = layers.Conv2D(1, 1, kernel_regularizer=l2(wt_decay))( tens )\n",
    "            class_preds = layers.Conv2D(len(cat_list), 1, kernel_regularizer=l2(wt_decay))( tens )\n",
    "            bbox_preds = layers.Conv2D(4, 1, kernel_regularizer=l2(wt_decay))( tens )\n",
    "            ar_preds.append( layers.Concatenate()([objectness_preds, class_preds, bbox_preds]) )\n",
    "\n",
    "        if num_aspect_ratios > 1:\n",
    "            predictions = layers.Concatenate()(ar_preds)\n",
    "        elif num_aspect_ratios == 1:\n",
    "            predictions = ar_preds[0]\n",
    "    \n",
    "        predictions = layers.Reshape( (*dims, num_aspect_ratios, 5+len(cat_list)), name=dimkey )(predictions)\n",
    "        preds_dict[dimkey] = predictions\n",
    "\n",
    "    model = keras.Model(inputs, preds_dict, name=model_name)\n",
    "\n",
    "    model.compile( optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "                 loss=custom_loss )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1043,
     "status": "ok",
     "timestamp": 1592545332716,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "fM0J24l-JcAg"
   },
   "outputs": [],
   "source": [
    "dims_list = [(7,7)]\n",
    "def transfer_model(backbone_model, input_shape, dims_list, num_aspect_ratios, wt_decay, model_name='transfer-objdet-model'):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    backbone_output = backbone_model(inputs) #7\n",
    "    \n",
    "    #backbone_output = VGG16(include_top=False,weights=\"imagenet\")(inputs)\n",
    "\n",
    "    x = layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay))(backbone_output) #7\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "\n",
    "    x = layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #5\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #5\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #3\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "\n",
    "    x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #3\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "    x = layers.Conv2D(128, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #3\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #3\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.01)(x)\n",
    "\n",
    "    # You can accumulate more scales via shortcut. Imagine each (n,m) is a grid super-imposed on the original image.\n",
    "    # See the next cell for an example for more scales.\n",
    "    dim_tensor_map = {'7x7': x}\n",
    "\n",
    "    # For each dimension, construct a predictions tensor. Accumulate them into a dictionary for keras to understand multiple labels.\n",
    "    preds_dict = {}\n",
    "    for dims in dims_list:\n",
    "        dimkey = '{}x{}'.format(*dims)\n",
    "        tens = dim_tensor_map[dimkey]\n",
    "        ar_preds = []\n",
    "        for _ in range(num_aspect_ratios):\n",
    "            objectness_preds = layers.Conv2D(1, 1, kernel_regularizer=l2(wt_decay))( tens )\n",
    "            class_preds = layers.Conv2D(len(cat_list), 1, kernel_regularizer=l2(wt_decay))( tens )\n",
    "            bbox_preds = layers.Conv2D(4, 1, kernel_regularizer=l2(wt_decay))( tens )\n",
    "            ar_preds.append( layers.Concatenate()([objectness_preds, class_preds, bbox_preds]) )\n",
    "\n",
    "        if num_aspect_ratios > 1:\n",
    "            predictions = layers.Concatenate()(ar_preds)\n",
    "        elif num_aspect_ratios == 1:\n",
    "            predictions = ar_preds[0]\n",
    "    \n",
    "        predictions = layers.Reshape( (*dims, num_aspect_ratios, 5+len(cat_list)), name=dimkey )(predictions)\n",
    "        preds_dict[dimkey] = predictions\n",
    "\n",
    "    model = keras.Model(inputs, preds_dict, name=model_name)\n",
    "\n",
    "    model.compile( optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "                 loss=custom_loss )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10018,
     "status": "ok",
     "timestamp": 1592545341883,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "sut7VnL2KEVK",
    "outputId": "9e956c81-4a9f-42b6-e335-f2b3ab0578d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 0s 0us/step\n",
      "Model: \"TeamMIA_cv_VGG16_wd0.0005-VGG16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vgg16 (Model)                   (None, 7, 7, 512)    14714688    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 7, 7, 512)    262656      vgg16[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 7, 7, 512)    2048        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 7, 7, 512)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 7, 7, 512)    2359808     leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 7, 7, 512)    2048        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 7, 7, 512)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 7, 7, 256)    131328      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 7, 7, 256)    1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 7, 7, 256)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 7, 7, 512)    1180160     leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 7, 7, 512)    2048        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 7, 7, 512)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 7, 7, 256)    131328      leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 7, 7, 256)    1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 7, 7, 256)    0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 7, 7, 128)    295040      leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 7, 7, 128)    512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 7, 7, 128)    0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 7, 7, 64)     73792       leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 7, 7, 64)     256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 7, 7, 64)     0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 7, 7, 1)      65          leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 7, 7, 5)      325         leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 7, 7, 4)      260         leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 7, 7, 1)      65          leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 7, 7, 5)      325         leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 7, 7, 4)      260         leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 7, 7, 1)      65          leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 7, 7, 5)      325         leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 7, 7, 4)      260         leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 7, 7, 10)     0           conv2d_7[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 7, 7, 10)     0           conv2d_10[0][0]                  \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 7, 7, 10)     0           conv2d_13[0][0]                  \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 7, 7, 30)     0           concatenate[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "7x7 (Reshape)                   (None, 7, 7, 3, 10)  0           concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 19,159,710\n",
      "Trainable params: 19,155,230\n",
      "Non-trainable params: 4,480\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Choose whether to start a new model \n",
    "# or load a previously trained one\n",
    "\n",
    "#model_context = 'model-7x7-14x14-3aspect-modyoloposneg-wd{}'.format(wt_decay)\n",
    "model_context = 'TeamMIA_cv_VGG16_wd{}'.format(wt_decay)\n",
    "\n",
    "#load_model_path = os.path.join( load_model_folder, '{}-best_val_loss.h5'.format(model_context) )\n",
    "load_model_path = None\n",
    "\n",
    "if load_model_path is None:\n",
    "    #backbone_model = tf.keras.applications.ResNet50(input_shape=input_shape, include_top=False)\n",
    "    #model = transfer_model(backbone_model, input_shape=input_shape, dims_list=dims_list, num_aspect_ratios=len(aspect_ratios), wt_decay=wt_decay, model_name=model_context+'-res50')\n",
    "    backbone_model = tf.keras.applications.VGG16(input_shape=input_shape, include_top=False)\n",
    "    model = transfer_model(backbone_model, input_shape=input_shape, dims_list=dims_list, num_aspect_ratios=len(aspect_ratios), wt_decay=wt_decay, model_name=model_context+'-VGG16')\n",
    "else:\n",
    "    model = tf.keras.models.load_model(load_model_path, custom_objects={'custom_loss':custom_loss})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B-8zEFSUKWhq"
   },
   "source": [
    "## Training/Transfer Learning of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25766,
     "status": "ok",
     "timestamp": 1592545357972,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "4W9qNyKBKIFQ",
    "outputId": "996b699c-fc61-4798-dfb3-df296722559b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training sequence...\n",
      "Creating validation sequence...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- There is overfitting now that I set top 25% (of each dim-ar combination) as positives. How?\n",
    "- Larger image size - maybe 448\n",
    "- Transfer learning\n",
    "- Change weights of losses?\n",
    "\n",
    "# Also add more callbacks, such as tensorboard \n",
    "dataset, batch_size, augment_fn, input_size, label_encoder, preprocess_fn\n",
    "encode_label(label_arr, dims_list, aspect_ratios, iou_fn, sampling_fn, cat_list)\n",
    "img_folder, json_annotation_file, batch_size, augment_fn, input_size, label_encoder, preprocess_fn\n",
    "'''\n",
    "\n",
    "bs=32\n",
    "n_epochs_warmup = 50\n",
    "n_epochs_after = 120\n",
    "\n",
    "label_encoder = lambda y: encode_label(y, dims_list, aspect_ratios, iou, modified_yolo_posneg_sampling, cat_list)\n",
    "preproc_fn = lambda x: x / 255.\n",
    "\n",
    "print('Creating training sequence...')\n",
    "#train_sequence = TILSequence(train_imgs_folder, train_annotations, bs, aug_default, input_shape, label_encoder, preproc_fn)\n",
    "train_sequence = TILPickle(train_pickle, bs, aug_default, input_shape, label_encoder, preproc_fn)\n",
    "print('Creating validation sequence...')\n",
    "#val_sequence = TILSequence(val_imgs_folder, val_annotations, bs, aug_identity, input_shape, label_encoder, preproc_fn)\n",
    "val_sequence = TILPickle(val_pickle, bs, aug_identity, input_shape, label_encoder, preproc_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 786,
     "status": "ok",
     "timestamp": 1592545362174,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "SRBSZy-iKXmh"
   },
   "outputs": [],
   "source": [
    "save_model_path = os.path.join(save_model_folder, '{}-best_val_loss.h5'.format(model_context))\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                                                filepath=save_model_path,\n",
    "                                                                save_weights_only=False,\n",
    "                                                                monitor='val_loss',\n",
    "                                                                mode='auto',\n",
    "                                                                save_best_only=True)\n",
    "\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12048556,
     "status": "ok",
     "timestamp": 1592456490664,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "7ykhJC9EKi9R",
    "outputId": "4bfeb9a3-64d6-489d-beb4-894ecac89940"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up the model...\n",
      "Epoch 1/50\n",
      "258/258 [==============================] - 246s 953ms/step - loss: 1.8317 - val_loss: 1.8043 - lr: 1.0000e-05\n",
      "Epoch 2/50\n",
      "258/258 [==============================] - 240s 929ms/step - loss: 1.5681 - val_loss: 1.5511 - lr: 1.0000e-05\n",
      "Epoch 3/50\n",
      "258/258 [==============================] - 239s 928ms/step - loss: 1.4885 - val_loss: 1.4584 - lr: 1.0000e-05\n",
      "Epoch 4/50\n",
      "258/258 [==============================] - 239s 927ms/step - loss: 1.4293 - val_loss: 1.4119 - lr: 1.0000e-05\n",
      "Epoch 5/50\n",
      "258/258 [==============================] - 239s 928ms/step - loss: 1.3767 - val_loss: 1.3450 - lr: 1.0000e-05\n",
      "Epoch 6/50\n",
      "258/258 [==============================] - 240s 932ms/step - loss: 1.3293 - val_loss: 1.2978 - lr: 1.0000e-05\n",
      "Epoch 7/50\n",
      "258/258 [==============================] - 239s 928ms/step - loss: 1.2820 - val_loss: 1.2517 - lr: 1.0000e-05\n",
      "Epoch 8/50\n",
      "258/258 [==============================] - 239s 927ms/step - loss: 1.2356 - val_loss: 1.2092 - lr: 1.0000e-05\n",
      "Epoch 9/50\n",
      "258/258 [==============================] - 239s 927ms/step - loss: 1.1936 - val_loss: 1.1684 - lr: 1.0000e-05\n",
      "Epoch 10/50\n",
      "258/258 [==============================] - 239s 927ms/step - loss: 1.1479 - val_loss: 1.1242 - lr: 1.0000e-05\n",
      "Epoch 11/50\n",
      "258/258 [==============================] - 240s 931ms/step - loss: 1.1038 - val_loss: 1.0775 - lr: 1.0000e-05\n",
      "Epoch 12/50\n",
      "258/258 [==============================] - 239s 927ms/step - loss: 1.0594 - val_loss: 1.0344 - lr: 1.0000e-05\n",
      "Epoch 13/50\n",
      "258/258 [==============================] - 239s 928ms/step - loss: 1.0152 - val_loss: 0.9869 - lr: 1.0000e-05\n",
      "Epoch 14/50\n",
      "258/258 [==============================] - 239s 928ms/step - loss: 0.9720 - val_loss: 0.9481 - lr: 1.0000e-05\n",
      "Epoch 15/50\n",
      "258/258 [==============================] - 239s 927ms/step - loss: 0.9293 - val_loss: 0.9041 - lr: 1.0000e-05\n",
      "Epoch 16/50\n",
      "258/258 [==============================] - 240s 930ms/step - loss: 0.8868 - val_loss: 0.8614 - lr: 1.0000e-05\n",
      "Epoch 17/50\n",
      "258/258 [==============================] - 239s 926ms/step - loss: 0.8455 - val_loss: 0.8230 - lr: 1.0000e-05\n",
      "Epoch 18/50\n",
      "258/258 [==============================] - 240s 929ms/step - loss: 0.8058 - val_loss: 0.7836 - lr: 1.0000e-05\n",
      "Epoch 19/50\n",
      "258/258 [==============================] - 239s 926ms/step - loss: 0.7669 - val_loss: 0.7480 - lr: 1.0000e-05\n",
      "Epoch 20/50\n",
      "258/258 [==============================] - 239s 926ms/step - loss: 0.7297 - val_loss: 0.7117 - lr: 1.0000e-05\n",
      "Epoch 21/50\n",
      "258/258 [==============================] - 239s 926ms/step - loss: 0.6940 - val_loss: 0.6760 - lr: 1.0000e-05\n",
      "Epoch 22/50\n",
      "258/258 [==============================] - 239s 927ms/step - loss: 0.6582 - val_loss: 0.6415 - lr: 1.0000e-05\n",
      "Epoch 23/50\n",
      "258/258 [==============================] - 239s 927ms/step - loss: 0.6239 - val_loss: 0.6070 - lr: 1.0000e-05\n",
      "Epoch 24/50\n",
      "258/258 [==============================] - 239s 926ms/step - loss: 0.5911 - val_loss: 0.5780 - lr: 1.0000e-05\n",
      "Epoch 25/50\n",
      "258/258 [==============================] - 239s 927ms/step - loss: 0.5588 - val_loss: 0.5427 - lr: 1.0000e-05\n",
      "Epoch 26/50\n",
      "258/258 [==============================] - 239s 926ms/step - loss: 0.5288 - val_loss: 0.5201 - lr: 1.0000e-05\n",
      "Epoch 27/50\n",
      "258/258 [==============================] - 239s 927ms/step - loss: 0.5021 - val_loss: 0.4903 - lr: 1.0000e-05\n",
      "Epoch 28/50\n",
      "258/258 [==============================] - 239s 928ms/step - loss: 0.4768 - val_loss: 0.4666 - lr: 1.0000e-05\n",
      "Epoch 29/50\n",
      "258/258 [==============================] - 239s 928ms/step - loss: 0.4524 - val_loss: 0.4449 - lr: 1.0000e-05\n",
      "Epoch 30/50\n",
      "258/258 [==============================] - 244s 945ms/step - loss: 0.4296 - val_loss: 0.4178 - lr: 1.0000e-05\n",
      "Epoch 31/50\n",
      "258/258 [==============================] - 239s 927ms/step - loss: 0.4074 - val_loss: 0.4000 - lr: 1.0000e-05\n",
      "Epoch 32/50\n",
      "258/258 [==============================] - 239s 928ms/step - loss: 0.3866 - val_loss: 0.3793 - lr: 1.0000e-05\n",
      "Epoch 33/50\n",
      "258/258 [==============================] - 239s 928ms/step - loss: 0.3668 - val_loss: 0.3599 - lr: 1.0000e-05\n",
      "Epoch 34/50\n",
      "258/258 [==============================] - 239s 927ms/step - loss: 0.3491 - val_loss: 0.3425 - lr: 1.0000e-05\n",
      "Epoch 35/50\n",
      "258/258 [==============================] - 239s 927ms/step - loss: 0.3322 - val_loss: 0.3268 - lr: 1.0000e-05\n",
      "Epoch 36/50\n",
      "258/258 [==============================] - 239s 927ms/step - loss: 0.3163 - val_loss: 0.3103 - lr: 1.0000e-05\n",
      "Epoch 37/50\n",
      "258/258 [==============================] - 239s 927ms/step - loss: 0.3011 - val_loss: 0.2973 - lr: 1.0000e-05\n",
      "Epoch 38/50\n",
      "258/258 [==============================] - 239s 927ms/step - loss: 0.2869 - val_loss: 0.2819 - lr: 1.0000e-05\n",
      "Epoch 39/50\n",
      "258/258 [==============================] - 240s 929ms/step - loss: 0.2740 - val_loss: 0.2693 - lr: 1.0000e-05\n",
      "Epoch 40/50\n",
      "258/258 [==============================] - 240s 929ms/step - loss: 0.2620 - val_loss: 0.2667 - lr: 1.0000e-05\n",
      "Epoch 41/50\n",
      "258/258 [==============================] - 240s 929ms/step - loss: 0.2515 - val_loss: 0.2497 - lr: 1.0000e-05\n",
      "Epoch 42/50\n",
      "258/258 [==============================] - 240s 929ms/step - loss: 0.2416 - val_loss: 0.2378 - lr: 1.0000e-05\n",
      "Epoch 43/50\n",
      "258/258 [==============================] - 240s 929ms/step - loss: 0.2326 - val_loss: 0.2299 - lr: 1.0000e-05\n",
      "Epoch 44/50\n",
      "258/258 [==============================] - 240s 929ms/step - loss: 0.2240 - val_loss: 0.2208 - lr: 1.0000e-05\n",
      "Epoch 45/50\n",
      "258/258 [==============================] - 240s 931ms/step - loss: 0.2153 - val_loss: 0.2126 - lr: 1.0000e-05\n",
      "Epoch 46/50\n",
      "258/258 [==============================] - 240s 930ms/step - loss: 0.2078 - val_loss: 0.2047 - lr: 1.0000e-05\n",
      "Epoch 47/50\n",
      "258/258 [==============================] - 240s 929ms/step - loss: 0.2004 - val_loss: 0.1980 - lr: 1.0000e-05\n",
      "Epoch 48/50\n",
      "258/258 [==============================] - 240s 930ms/step - loss: 0.1933 - val_loss: 0.1917 - lr: 1.0000e-05\n",
      "Epoch 49/50\n",
      "258/258 [==============================] - 240s 929ms/step - loss: 0.1867 - val_loss: 0.1830 - lr: 1.0000e-05\n",
      "Epoch 50/50\n",
      "258/258 [==============================] - 240s 930ms/step - loss: 0.1802 - val_loss: 0.1770 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f01f4def4e0>"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Warming up the model...')\n",
    "model.fit(x=train_sequence, \n",
    "          epochs=n_epochs_warmup, \n",
    "          validation_data=val_sequence,\n",
    "          callbacks=[model_checkpoint_callback, earlystopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 834,
     "status": "ok",
     "timestamp": 1592545366980,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "3Pn57U3FG2jw"
   },
   "outputs": [],
   "source": [
    "#n_epochs_after = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2044375,
     "status": "ok",
     "timestamp": 1592547411877,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "v8R3xiNAKlSX",
    "outputId": "38ccdadb-197f-4be4-af55-9308066e57f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model warmed. Loading best val version of model...\n",
      "Epoch 1/30\n",
      "258/258 [==============================] - 73s 283ms/step - loss: 0.1736 - val_loss: 0.1712 - lr: 1.0000e-05\n",
      "Epoch 2/30\n",
      "258/258 [==============================] - 67s 258ms/step - loss: 0.1674 - val_loss: 0.1650 - lr: 1.0000e-05\n",
      "Epoch 3/30\n",
      "258/258 [==============================] - 67s 258ms/step - loss: 0.1622 - val_loss: 0.1592 - lr: 1.0000e-05\n",
      "Epoch 4/30\n",
      "258/258 [==============================] - 67s 259ms/step - loss: 0.1573 - val_loss: 0.1527 - lr: 1.0000e-05\n",
      "Epoch 5/30\n",
      "258/258 [==============================] - 67s 259ms/step - loss: 0.1528 - val_loss: 0.1496 - lr: 1.0000e-05\n",
      "Epoch 6/30\n",
      "258/258 [==============================] - 67s 258ms/step - loss: 0.1483 - val_loss: 0.1452 - lr: 1.0000e-05\n",
      "Epoch 7/30\n",
      "258/258 [==============================] - 66s 258ms/step - loss: 0.1446 - val_loss: 0.1408 - lr: 1.0000e-05\n",
      "Epoch 8/30\n",
      "258/258 [==============================] - 67s 258ms/step - loss: 0.1406 - val_loss: 0.1380 - lr: 1.0000e-05\n",
      "Epoch 9/30\n",
      "258/258 [==============================] - 67s 260ms/step - loss: 0.1370 - val_loss: 0.1332 - lr: 1.0000e-05\n",
      "Epoch 10/30\n",
      "258/258 [==============================] - 67s 258ms/step - loss: 0.1339 - val_loss: 0.1314 - lr: 1.0000e-05\n",
      "Epoch 11/30\n",
      "258/258 [==============================] - 67s 260ms/step - loss: 0.1308 - val_loss: 0.1273 - lr: 1.0000e-05\n",
      "Epoch 12/30\n",
      "258/258 [==============================] - 67s 258ms/step - loss: 0.1278 - val_loss: 0.1229 - lr: 1.0000e-05\n",
      "Epoch 13/30\n",
      "258/258 [==============================] - 67s 260ms/step - loss: 0.1250 - val_loss: 0.1228 - lr: 1.0000e-05\n",
      "Epoch 14/30\n",
      "258/258 [==============================] - 67s 259ms/step - loss: 0.1225 - val_loss: 0.1183 - lr: 1.0000e-05\n",
      "Epoch 15/30\n",
      "258/258 [==============================] - 67s 259ms/step - loss: 0.1199 - val_loss: 0.1166 - lr: 1.0000e-05\n",
      "Epoch 16/30\n",
      "258/258 [==============================] - 67s 259ms/step - loss: 0.1173 - val_loss: 0.1142 - lr: 1.0000e-05\n",
      "Epoch 17/30\n",
      "258/258 [==============================] - 67s 258ms/step - loss: 0.1155 - val_loss: 0.1111 - lr: 1.0000e-05\n",
      "Epoch 18/30\n",
      "258/258 [==============================] - 67s 259ms/step - loss: 0.1133 - val_loss: 0.1091 - lr: 1.0000e-05\n",
      "Epoch 19/30\n",
      "258/258 [==============================] - 67s 259ms/step - loss: 0.1111 - val_loss: 0.1055 - lr: 1.0000e-05\n",
      "Epoch 20/30\n",
      "258/258 [==============================] - 67s 258ms/step - loss: 0.1092 - val_loss: 0.1053 - lr: 1.0000e-05\n",
      "Epoch 21/30\n",
      "258/258 [==============================] - 67s 258ms/step - loss: 0.1073 - val_loss: 0.1039 - lr: 1.0000e-05\n",
      "Epoch 22/30\n",
      "258/258 [==============================] - 67s 259ms/step - loss: 0.1055 - val_loss: 0.1011 - lr: 1.0000e-05\n",
      "Epoch 23/30\n",
      "258/258 [==============================] - 67s 259ms/step - loss: 0.1038 - val_loss: 0.0996 - lr: 1.0000e-05\n",
      "Epoch 24/30\n",
      "258/258 [==============================] - 67s 261ms/step - loss: 0.1020 - val_loss: 0.0972 - lr: 1.0000e-05\n",
      "Epoch 25/30\n",
      "258/258 [==============================] - 67s 259ms/step - loss: 0.1005 - val_loss: 0.0961 - lr: 1.0000e-05\n",
      "Epoch 26/30\n",
      "258/258 [==============================] - 67s 259ms/step - loss: 0.0990 - val_loss: 0.0944 - lr: 1.0000e-05\n",
      "Epoch 27/30\n",
      "258/258 [==============================] - 67s 259ms/step - loss: 0.0979 - val_loss: 0.0941 - lr: 1.0000e-05\n",
      "Epoch 28/30\n",
      "258/258 [==============================] - 67s 260ms/step - loss: 0.0966 - val_loss: 0.0920 - lr: 1.0000e-05\n",
      "Epoch 29/30\n",
      "258/258 [==============================] - 67s 258ms/step - loss: 0.0958 - val_loss: 0.0898 - lr: 1.0000e-05\n",
      "Epoch 30/30\n",
      "258/258 [==============================] - 67s 259ms/step - loss: 0.0939 - val_loss: 0.0891 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Fine tuning\n",
    "print('Model warmed. Loading best val version of model...')\n",
    "load_model_path = os.path.join( load_model_folder, '{}-best_val_loss.h5'.format(model_context) )\n",
    "del model\n",
    "model = tf.keras.models.load_model(load_model_path, custom_objects={'custom_loss':custom_loss})\n",
    "\n",
    "#for layer in model.get_layer('VGG16').layers:\n",
    " #   layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss=custom_loss)\n",
    "model_context = 'ft-' + model_context\n",
    "save_model_path = os.path.join( save_model_folder, '{}-best_val_loss.h5'.format(model_context) )\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                                                filepath=save_model_path,\n",
    "                                                                save_weights_only=False,\n",
    "                                                                monitor='val_loss',\n",
    "                                                                mode='auto',\n",
    "                                                                save_best_only=True)\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-8)\n",
    "\n",
    "model.fit(x=train_sequence, \n",
    "          epochs=n_epochs_after, \n",
    "          validation_data=val_sequence, \n",
    "          callbacks=[model_checkpoint_callback, earlystopping, reduce_lr])\n",
    "\n",
    "# Final save\n",
    "model.save(os.path.join(save_model_folder, '{}-final.h5'.format(model_context)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VtzLZ4MruJVM"
   },
   "source": [
    "## Non-max suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 822,
     "status": "ok",
     "timestamp": 1592548679586,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "Rz2w3a_il9ba"
   },
   "outputs": [],
   "source": [
    "# To fix multiple, we introduce non-maximum suppression, or NMS for short\n",
    "def nms(detections, iou_thresh=0.):\n",
    "  dets_by_class = {}\n",
    "  final_result = []\n",
    "  for det in detections:\n",
    "    cls = det[1]\n",
    "    if cls not in dets_by_class:\n",
    "      dets_by_class[cls] = []\n",
    "    dets_by_class[cls].append( det )\n",
    "  for _, dets in dets_by_class.items():\n",
    "    candidates = list(dets)\n",
    "    candidates.sort( key=lambda x:x[0], reverse=True )\n",
    "    while len(candidates) > 0:\n",
    "      candidate = candidates.pop(0)\n",
    "      _,_,cx,cy,cw,ch = candidate\n",
    "      copy = list(candidates)\n",
    "      for other in candidates:\n",
    "        # Compute the IoU. If it exceeds thresh, we remove it\n",
    "        _,_,ox,oy,ow,oh = other\n",
    "        if iou( (cx,cy,cw,ch), (ox,oy,ow,oh) ) > iou_thresh:\n",
    "          copy.remove(other)\n",
    "      candidates = list(copy)\n",
    "      final_result.append(candidate)\n",
    "  return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DG7zDN-PuTo6"
   },
   "source": [
    "## Load a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2235,
     "status": "ok",
     "timestamp": 1592548776205,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "NBvbAWSVuRDs"
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "#load_model_path = os.path.join( load_model_folder, 'model-7x7-14x14-3aspect-modyoloposneg-wd0.0005-best_val_loss.h5' )\n",
    "load_model_path = os.path.join( load_model_folder, 'TeamMIA_cv_VGG16_wd0.0005-best_val_loss.h5' )\n",
    "model = tf.keras.models.load_model(load_model_path, custom_objects={'custom_loss':custom_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KyU-iIGaw32o"
   },
   "source": [
    "## Load the validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2369,
     "status": "ok",
     "timestamp": 1592548782306,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "85U566AJuZZJ"
   },
   "outputs": [],
   "source": [
    "# load the validation data\n",
    "label_encoder = lambda y: encode_label(y, dims_list, aspect_ratios, iou, modified_yolo_posneg_sampling, cat_list)\n",
    "preproc_fn = lambda x: x / 255.\n",
    "\n",
    "val_sequence_pickle = TILPickle(val_pickle, 1, aug_identity, input_shape, label_encoder, preproc_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 319433,
     "status": "ok",
     "timestamp": 1592549102850,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "y9ZYrvJyWNwO"
   },
   "outputs": [],
   "source": [
    "val_sequence = TILSequence(val_imgs_folder, val_annotations, 1, aug_identity, input_shape, label_encoder, preproc_fn, testmode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_42ECKouX4y"
   },
   "source": [
    "## Visualize Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1Q_B8TOgudkTcT30SoC9BSaV8FUWRKrnT"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5836,
     "status": "ok",
     "timestamp": 1592550336286,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "v93eWhNZud3P",
    "outputId": "c3df5a17-0862-4f38-8af8-d1bedcfba6b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this to visualize\n",
    "rank_colors = ['cyan', 'magenta', 'pink']\n",
    "det_threshold=0.\n",
    "top_dets=3\n",
    "\n",
    "start=0\n",
    "end=20\n",
    "for k in range(start,end):\n",
    "  img_arr, label_cxywh = val_sequence_pickle[k]\n",
    "  img_arr = img_arr[0]\n",
    "  pil_img = PIL.Image.fromarray( (img_arr * 255.).astype(np.uint8) )\n",
    "  W,H = pil_img.size\n",
    "  pred_dict = model(np.array([img_arr]))\n",
    "  preds = decode_tensor( pred_dict, aspect_ratios )\n",
    "    \n",
    "  # Post-processing\n",
    "  preds.sort( key=lambda x:x[0], reverse=True )\n",
    "  preds = [pred for pred in preds if pred[0] >= det_threshold]\n",
    "  preds = preds[:top_dets]\n",
    "  preds = nms(preds, iou_thresh=0.5)\n",
    "\n",
    "  draw_img = pil_img.copy()\n",
    "  draw = ImageDraw.Draw(draw_img)\n",
    "  for i, pred in enumerate(preds):\n",
    "    conf,cls,x,y,w,h = pred\n",
    "    bb_x = int(x * W)\n",
    "    bb_y = int(y * H)\n",
    "    bb_w = int(w * W)\n",
    "    bb_h = int(h * H)\n",
    "    left = int(bb_x - bb_w / 2)\n",
    "    top = int(bb_y - bb_h / 2)\n",
    "    right = int(bb_x + bb_w / 2)\n",
    "    bot = int(bb_y + bb_h / 2)\n",
    "    cls_str = cat_list[cls-1]\n",
    "\n",
    "    draw.rectangle(((left, top), (right, bot)), outline=rank_colors[i])\n",
    "    draw.text((bb_x, bb_y), cls_str, fill=rank_colors[i])\n",
    "    draw.text( ( int(left + bb_w*.1), int(top + bb_h*.1) ), '{:.2f}'.format(conf), fill=rank_colors[i] )\n",
    "\n",
    "  display(draw_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1744,
     "status": "ok",
     "timestamp": 1592550363020,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "cU7iPUTkzg6P"
   },
   "outputs": [],
   "source": [
    "# JSON FILE NAMING CONSTANT\n",
    "#VALIDATION_JSON = 'detections-7x7-14x14-top100-nonms.json'\n",
    "VALIDATION_JSON = 'detections_val.json'\n",
    "TEST_JSON = 'detections_interim.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODTMSSvpztT7"
   },
   "source": [
    "## Generating detections on the folder of validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 520275,
     "status": "ok",
     "timestamp": 1592552333254,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "6Xu57IkMR1jK",
    "outputId": "dffe68e8-ef53-4a52-875d-5b4ddb8530c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1474/1474 [08:37<00:00,  2.85it/s]\n"
     ]
    }
   ],
   "source": [
    "detections = []\n",
    "det_threshold=0.\n",
    "for i in tqdm(range(len(val_sequence))):\n",
    "  img_id, dims, input_arr, _ = val_sequence[i]\n",
    "  img_id = int(img_id[0])\n",
    "  W,H = dims[0]\n",
    "\n",
    "  # Here, I'm inferencing one-by-one, but you can batch it if you want it faster\n",
    "  pred_dict = model(input_arr)\n",
    "  preds = decode_tensor( pred_dict, aspect_ratios )\n",
    "\n",
    "  # Post-processing\n",
    "  preds = [pred for pred in preds if pred[0] >= det_threshold]\n",
    "  preds.sort( key=lambda x:x[0], reverse=True )\n",
    "  preds = preds[:100] # we only evaluate you on 100 detections per image\n",
    "\n",
    "  for i, pred in enumerate(preds):\n",
    "    conf,cat_id,x,y,w,h = pred\n",
    "    left = W * (x - w/2.)\n",
    "    left = round(left,1)\n",
    "    top = H * (y - h/2.)\n",
    "    top = round(top,1)\n",
    "    width = W*w\n",
    "    width = round(width,1)\n",
    "    height = H*h\n",
    "    height = round(height,1)\n",
    "    conf = float(conf)\n",
    "    cat_id = int(cat_id)\n",
    "    detections.append( {'image_id':img_id, 'category_id':cat_id, 'bbox':[left, top, width, height], 'score':conf} )\n",
    "\n",
    "with open(VALIDATION_JSON, 'w') as f:\n",
    "  json.dump(detections, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9970,
     "status": "ok",
     "timestamp": 1592552416437,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "iMOqbqfkSJKO",
    "outputId": "6c94e938-7d6d-45ed-b838-00ed113d6bc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/jinmingteo/cocoapi.git#subdirectory=PythonAPI\n",
      "  Cloning https://github.com/jinmingteo/cocoapi.git to /tmp/pip-req-build-bxeos3_w\n",
      "  Running command git clone -q https://github.com/jinmingteo/cocoapi.git /tmp/pip-req-build-bxeos3_w\n",
      "Requirement already satisfied (use --upgrade to upgrade): pycocotools==2.0 from git+https://github.com/jinmingteo/cocoapi.git#subdirectory=PythonAPI in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (47.3.1)\n",
      "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (0.29.20)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (3.2.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.17.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools==2.0) (1.12.0)\n",
      "Building wheels for collected packages: pycocotools\n",
      "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=267064 sha256=14bf2f7ef7f7657344932b44cfc4d02aa3bf1db2d66057a0b8ca11a3f31ea603\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-mrr323uv/wheels/27/81/92/3a512329d1b1ae7fc278285a1f114ef08082568bf32eee0002\n",
      "Successfully built pycocotools\n"
     ]
    }
   ],
   "source": [
    "#This installation is a modified version of the original to suit this competition\n",
    "! pip install git+https://github.com/jinmingteo/cocoapi.git#subdirectory=PythonAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1101,
     "status": "ok",
     "timestamp": 1592552421383,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "FnEFeuvUdiKK"
   },
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9815,
     "status": "ok",
     "timestamp": 1592552431844,
     "user": {
      "displayName": "Nicholas Neo",
      "photoUrl": "",
      "userId": "17512883430045220190"
     },
     "user_tz": -480
    },
    "id": "amv4RY1Ndo2H",
    "outputId": "48796750-d796-4fe9-ec8c-a7943bcfcec2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=1.16s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=6.28s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=1.48s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.002\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.007\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.002\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.004\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.027\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.056\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.010\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.056\n"
     ]
    }
   ],
   "source": [
    "# Get evaluation score against validation set\n",
    "coco_gt = COCO(val_annotations)\n",
    "coco_dt = coco_gt.loadRes('./gdrive/My Drive/datasets/submissions/' + VALIDATION_JSON)\n",
    "cocoEval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jR0OmAaHdwbH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNA37IL59bjSNGrLGRYv+1M",
   "collapsed_sections": [],
   "mount_file_id": "19G7Cy8hnmTg1vUqFt-pKc5Ypce8hhKzS",
   "name": "CV-v3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
